{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af1d4a2a-bff3-4c2f-9aa6-71905a25e42a",
   "metadata": {},
   "source": [
    "## Q1. What is the curse of dimensionality reduction and why is it important in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0899eadc-0b65-4ace-8cc5-8fc1f00d4258",
   "metadata": {},
   "source": [
    "Ans-Curse of Dimensionality refers to a set of problems that arise when working with high-dimensional data. The dimension of a dataset corresponds to the number of attributes/features that exist in a dataset. A dataset with a large number of attributes, generally of the order of a hundred or more, is referred to as high dimensional data. Some of the difficulties that come with high dimensional data manifest during analyzing or visualizing the data to identify patterns, and some manifest while training machine learning models. The difficulties related to training machine learning models due to high dimensional data are referred to as the ‘Curse of Dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584eec6a-8b43-45dc-9cca-e7d7342f46e8",
   "metadata": {},
   "source": [
    "Machine Learning\n",
    "In Machine Learning, a marginal increase in dimensionality also requires a large increase in the volume in the data in order to maintain the same level of performance. The curse of dimensionality is the by-product of a phenomenon which appears with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7f6fdc-edd0-4796-9c7a-794b6dfc764b",
   "metadata": {},
   "source": [
    "## Q2. How does the curse of dimensionality impact the performance of machine learning algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e4bb35-edb2-4fe3-aaef-48ab57b39829",
   "metadata": {},
   "source": [
    "Ans-As the number of dimensions or features increases, the amount of data needed to generalize the machine learning model accurately increases exponentially. The increase in dimensions makes the data sparse, and it increases the difficulty of generalizing the model. More training data is needed to generalize that model better.\n",
    "\n",
    "The higher dimensions lead to equidistant separation between points. The higher the dimensions, the more difficult it will be to sample from because the sampling loses its randomness.\n",
    "\n",
    "It becomes harder to collect observations if there are plenty of features. These dimensions make all observations in the dataset to be equidistant from all other observations. The clustering uses Euclidean distance to measure the similarity between the observations. The meaningful clusters can’t be formed if the distances are equidistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7521155-8301-4343-a1b1-0fcf1e737f37",
   "metadata": {},
   "source": [
    "## Q3. What are some of the consequences of the curse of dimensionality in machine learning, and how do they impact model performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783d246b-d36d-4355-9006-9049875a397c",
   "metadata": {},
   "source": [
    "Ans-Effect of Curse of Dimensionality on Distance Functions:\n",
    "Therefore, any machine learning algorithms which are based on the distance measure including KNN(k-Nearest Neighbor) tend to fail when the number of dimensions in the data is very high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a2b9817-8be5-4de9-8a41-f4f191034ffd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9bdc4e1-8d15-4852-addf-5482fa651c5a",
   "metadata": {},
   "source": [
    "## Q4. Can you explain the concept of feature selection and how it can help with dimensionality reduction?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f10dcbc-0739-4b88-9600-3604d4599cbc",
   "metadata": {},
   "source": [
    "Ans-There are two components of dimensionality reduction:\n",
    "\n",
    "Feature selection: In this, we try to find a subset of the original set of variables, or features, to get a smaller subset which can be used to model the problem. It usually involves three ways:\n",
    "Filter\n",
    "Wrapper\n",
    "Embedded\n",
    "Feature extraction: This reduces the data in a high dimensional space to a lower dimension space, i.e. a space with lesser no. of dimensions.\n",
    "Methods of Dimensionality Reduction\n",
    "\n",
    "The various methods used for dimensionality reduction include:\n",
    "\n",
    "Principal Component Analysis (PCA)\n",
    "Linear Discriminant Analysis (LDA)\n",
    "Generalized Discriminant Analysis (GDA)\n",
    "Dimensionality reduction may be both linear and non-linear, depending upon the method used. The prime linear method, called Principal Component Analysis, or PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583b12e2-8147-4558-9f2e-e13627970161",
   "metadata": {},
   "source": [
    "Dimensionality reduction is the process of reducing the number of features in a dataset while retaining as much information as possible.\n",
    "This can be done to reduce the complexity of a model, improve the performance of a learning algorithm, or make it easier to visualize the data.\n",
    "Techniques for dimensionality reduction include: principal component analysis (PCA), singular value decomposition (SVD), and linear discriminant analysis (LDA).\n",
    "Each technique projects the data onto a lower-dimensional space while preserving important information.\n",
    "Dimensionality reduction is performed during pre-processing stage before building a model to improve the performance\n",
    "It is important to note that dimensionality reduction can also discard useful information, so care must be taken when applying these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c8f65c8-19a8-4ff7-a92a-3f0c34c0941a",
   "metadata": {},
   "source": [
    "## Q5. What are some limitations and drawbacks of using dimensionality reduction techniques in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c878ad97-91b8-482d-8d68-9b94069f7411",
   "metadata": {},
   "source": [
    "Ans-Dimensionality reduction brings many advantages to your machine learning data, including:\n",
    "\n",
    "Fewer features mean less complexity\n",
    "\n",
    "You will need less storage space because you have fewer data\n",
    "\n",
    "Fewer features require less computation time\n",
    "\n",
    "Model accuracy improves due to less misleading data\n",
    "\n",
    "Algorithms train faster thanks to fewer data\n",
    "\n",
    "Reducing the data set’s feature dimensions helps visualize the data faster\n",
    "\n",
    "It removes noise and redundant features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f00893-357c-426c-b86d-6c4f2056a685",
   "metadata": {},
   "source": [
    "Benefits Of Dimensionality Reduction\n",
    "For AI engineers or data professionals working with enormous datasets, doing data visualisation, and analysing complicated data, dimension reduction is helpful. \n",
    "\n",
    "1.It aids in data compression, resulting in less storage space being required.\n",
    "\n",
    "2.It speeds up the calculation.\n",
    "\n",
    "3.It also aids in removing any extraneous features.\n",
    "\n",
    "\n",
    "Disadvantages Of Dimensionality Reduction:\n",
    "\n",
    "1.We lost some data during the dimensionality reduction process, which can impact how well future training algorithms work.\n",
    "\n",
    "2.It may need a lot of processing power.\n",
    "\n",
    "3.Interpreting transformed characteristics might be challenging.\n",
    "\n",
    "4.The independent variables become harder to comprehend as a result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a33ac0-4645-436e-a522-5fc7cec0b390",
   "metadata": {},
   "source": [
    "## Q6. How does the curse of dimensionality relate to overfitting and underfitting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e838e9-2934-4c07-a847-a6d9bd95faa7",
   "metadata": {},
   "source": [
    "Ans-Overfitting and Underfitting\n",
    "There is a relationship between 'd' and overfitting which is as follows: 'd' is directly proportional to overfitting i.e. as the dimensionality increases the chances of overfitting also increases.\n",
    "\n",
    "a) Model-dependent approach: Whenever we have a large number of features, we can always perform forward feature selection to determine the most relevant features for the prediction.\n",
    "\n",
    "b) Unlike the above solution which is classification-oriented, we can also perform dimensionality reduction techniques like PCA and t-SNE which do not use the class labels to determine the most relevant features for the prediction.\n",
    "\n",
    "So it is important to keep in mind whenever you download a new dataset that has a large number of features, you can reduce it by some of the techniques like PCA, t-SNE, or forward selection in order to ensure your model is not affected by the curse of dimensionality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83c39bec-2941-48cb-bc75-c332af115458",
   "metadata": {},
   "source": [
    "## Q7. How can one determine the optimal number of dimensions to reduce data to when using dimensionality reduction techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0277269-5e97-4aad-b72a-f1443974cb60",
   "metadata": {},
   "source": [
    "Ans-ommon Dimensionality Reduction Techniques\n",
    "Dimensionality reduction can be done in two different ways:\n",
    "\n",
    "By only keeping the most relevant variables from the original dataset (this technique is called feature selection)\n",
    "By finding a smaller set of new variables, each being a combination of the input variables, containing basically the same information as the input variables (this technique is called dimensionality reduction)\n",
    "We will now look at various dimensionality reduction techniques and how to implement each of them in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35e82b6-9ee4-421c-9fc5-235a3b7e40c3",
   "metadata": {},
   "source": [
    "Missing Value Ratio\n",
    "Suppose you’re given a dataset. What would be your first step? You would naturally want to explore the data first before building model. While exploring the data, you find that your dataset has some missing values. Now what? You will try to find out the reason for these missing values and then impute them or drop the variables entirely which have missing values (using appropriate methods).\n",
    "\n",
    "What if we have too many missing values (say more than 50%)? Should we impute the missing values or drop the variable? I would prefer to drop the variable since it will not have much information. However, this isn’t set in stone. We can set a threshold value and if the percentage of missing values in any variable is more than that threshold, we will drop the variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a31c7b8-2cc7-4b8f-bc4b-1a560a70787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86244698-5417-4e13-a56e-cd49488b6730",
   "metadata": {},
   "source": [
    "# read the data\n",
    "train=pd.read_csv(\"Train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad47469-c418-47a4-b435-3a29130ae2bc",
   "metadata": {},
   "source": [
    "# checking the percentage of missing values in each variable\n",
    "train.isnull().sum()/len(train)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b9bba4-6d10-4677-b54c-a7475ddc7007",
   "metadata": {},
   "source": [
    "# saving missing values in a variable\n",
    "a = train.isnull().sum()/len(train)*100\n",
    "# saving column names in a variable\n",
    "variables = train.columns\n",
    "variable = [ ]\n",
    "for i in range(0,12):\n",
    "    if a[i]<=20:   #setting the threshold as 20%\n",
    "        variable.append(variables[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ffaf59-e089-405f-9579-d4f5db808b80",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a765a1-19c3-489c-8dc7-915ed957de7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0078c0dc-8197-4ff4-9de3-099a9e7472de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
