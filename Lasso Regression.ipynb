{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be531cef-b685-42c1-99b1-066f44faba91",
   "metadata": {},
   "source": [
    "## Q1. What is Lasso Regression, and how does it differ from other regression techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd24e5-c0f6-416e-9c8c-b0c0a988b88c",
   "metadata": {},
   "source": [
    "Ans-Lasso regression is a regularization technique. It is used over regression methods for a more accurate prediction. This model uses shrinkage. Shrinkage is where data values are shrunk towards a central point as the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of multicollinearity or when you want to automate certain parts of model selection, like variable selection/parameter elimination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0853f4e3-3dae-4b2d-9eaa-613f08103a6e",
   "metadata": {},
   "source": [
    "The word “LASSO” stands for Least Absolute Shrinkage and Selection Operator. It is a statistical formula for the regularisation of data models and feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd51379-cc02-4f87-9279-4532964d0167",
   "metadata": {},
   "source": [
    "Lasso Regularization Techniques\n",
    "There are two main regularization techniques, namely Ridge Regression and Lasso Regression. They both differ in the way they assign a penalty to the coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa9cd997-7778-4a0e-bf3a-ef9ff9ffd219",
   "metadata": {},
   "source": [
    "L1 Regularization\n",
    "If a regression model uses the L1 Regularization technique, then it is called Lasso Regression. If it used the L2 regularization technique, it’s called Ridge Regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd4d996-5058-47b8-8b43-aa8375a4b9b6",
   "metadata": {},
   "source": [
    "Mathematical equation of Lasso Regression\n",
    "Residual Sum of Squares + λ * (Sum of the absolute value of the magnitude of coefficients)\n",
    "\n",
    "Where,\n",
    "\n",
    "λ denotes the amount of shrinkage.\n",
    "λ = 0 implies all features are considered and it is equivalent to the linear regression where only the residual sum of squares is considered to build a predictive model\n",
    "λ = ∞ implies no feature is considered i.e, as λ closes to infinity it eliminates more and more features\n",
    "The bias increases with increase in λ\n",
    "variance increases with decrease in λ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016583bd-f752-4eeb-9ea9-bb36073904a2",
   "metadata": {},
   "source": [
    "import numpy as np #Creating a New Train and Validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd909b07-f795-45b7-b884-150c171d97d5",
   "metadata": {},
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "data_train, data_val = train_test_split(new_data_train, test_size = 0.2, random_state = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cec312e-5be2-473d-be45-fb42fe0c4540",
   "metadata": {},
   "source": [
    "# Classifying Predictors and Target\n",
    "\n",
    "#Classifying Independent and Dependent Features\n",
    "#_______________________________________________\n",
    "#Dependent Variable\n",
    "Y_train = data_train.iloc[:, -1].values\n",
    "#Independent Variables\n",
    "X_train = data_train.iloc[:,0 : -1].values\n",
    "#Independent Variables for Test Set\n",
    "X_test = data_val.iloc[:,0 : -1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0dbb0d-5b63-4883-8569-b73675c8c169",
   "metadata": {},
   "source": [
    "# Evaluating The Model With RMLSE\n",
    "\n",
    "def score(y_pred, y_true):\n",
    "error = np.square(np.log10(y_pred +1) - np.log10(y_true +1)).mean() ** 0.5\n",
    "score = 1 - error\n",
    "return score\n",
    "actual_cost = list(data_val['COST'])\n",
    "actual_cost = np.asarray(actual_cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b526aa8-ebfe-410b-a2a8-9f19a9e081d6",
   "metadata": {},
   "source": [
    "# Building the Lasso Regressor\n",
    "\n",
    "#Lasso Regression\n",
    "\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "#Initializing the Lasso Regressor with Normalization Factor as True\n",
    "lasso_reg = Lasso(normalize=True)\n",
    "#Fitting the Training data to the Lasso regressor\n",
    "lasso_reg.fit(X_train,Y_train)\n",
    "#Predicting for X_test\n",
    "y_pred_lass =lasso_reg.predict(X_test)\n",
    "#Printing the Score with RMLSE\n",
    "print(\"\\n\\nLasso SCORE : \", score(y_pred_lass, actual_cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445d47a5-035c-43a0-a4ca-ea0dff932a54",
   "metadata": {},
   "source": [
    "## Q2. What is the main advantage of using Lasso Regression in feature selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae0087b-3f36-4c1f-b94b-ae4dfd6f6a80",
   "metadata": {},
   "source": [
    "Ans-The main advantage of a LASSO regression model is that it has the ability to set the coefficients for features it does not consider interesting to zero. This means that the model does some automatic feature selection to decide which features should and should not be included on its own.\n",
    "ASSO offers models with high prediction accuracy. The accuracy increases since the method includes shrinkage of coefficients, which reduces variance and minimizes bias. It performs best when the number of observations is low and the number of features is high"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecbc8a4-76c3-40f9-adfd-5c12a8cc18d4",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the coefficients of a Lasso Regression model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655e8e83-8b56-4153-aacc-690d17c9ff1f",
   "metadata": {},
   "source": [
    "Ans-Interpretations and Generalizations\n",
    "# Interpretations:\n",
    "\n",
    "1.Geometric Interpretations\n",
    "2.Bayesian Interpretations\n",
    "3.Convex relaxation Interpretations\n",
    "4.Making λ easier to interpret with an accuracy-simplicity tradeoff\n",
    "Generalizations\n",
    "\n",
    "1.Elastic Net\n",
    "2.Group Lasso\n",
    "3.Fused Lasso\n",
    "4.Adaptive Lasso\n",
    "5.Prior Lasso\n",
    "6.Quasi-norms and bridge regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5d1d76-3b33-4028-8c97-0c66973a1cd3",
   "metadata": {},
   "source": [
    "## Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the\n",
    "# model's performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ddaf94-3a3d-4416-8598-8524f7f2e11d",
   "metadata": {},
   "source": [
    "Ans-A tuning parameter (λ), sometimes called a penalty parameter, controls the strength of the penalty term in ridge regression and lasso regression. It is basically the amount of shrinkage, where data values are shrunk towards a central point, like the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffa5da4-6d30-4139-ba5d-52d383117498",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "lassoReg = Lasso(alpha=0.3, normalize=True)\n",
    "lassoReg.fit(x_train,y_train)\n",
    "pred = lassoReg.predict(x_cv)\n",
    "# calculating mse\n",
    "mse = np.mean((pred_cv - y_cv)**2)\n",
    "mse\n",
    "1346205.82\n",
    "lassoReg.score(x_cv,y_cv)\n",
    "0.5720"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad5514f-72c0-4124-b759-92d06ac59a26",
   "metadata": {},
   "source": [
    "both the mse and the value of R-square for our model has been increased. Therefore, lasso model is predicting better than both linear and ridge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a3f33b-dc95-4d09-a718-0e090f6d9081",
   "metadata": {},
   "source": [
    "While Lasso regression can be a powerful technique for feature selection and regularization in linear regression models, it may not always be the best choice for every situation. Here are some situations where Lasso regression may not be the best approach:\n",
    "\n",
    "Small sample sizes\n",
    "Nonlinear relationships\n",
    "Correlated predictors\n",
    "Categorical predictors\n",
    "Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb38fc90-6298-41e0-b35f-dc2711d63b1d",
   "metadata": {},
   "source": [
    "## Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f5b653-ecd6-417a-9b80-2bd20c00ff50",
   "metadata": {},
   "source": [
    "Ans-The ordinary lasso penalty has been extensively used in the framework of linear regression models; however, sufficient results have not been obtained for nonlinear regression models with Gaussian basis functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067217bd-3f7a-4a08-96dd-c93085d0dc72",
   "metadata": {},
   "source": [
    "#The equation for Lasso regression can be expressed as:\n",
    " minimize RSS + λ * ||β||₁\n",
    " subject to ∑ᵢ |βᵢ| <= t\n",
    " \n",
    " 1.RSS is the residual sum of squares (i.e., the sum of the squared differences between the predicted and actual values)\n",
    "2.β is the vector of regression coefficients\n",
    "λ is the regularization parameter, which controls the strength of the penalty on the absolute values of the coefficients\n",
    "3.||.||₁ represents the L1 norm, which is simply the sum of the absolute values of the coefficients\n",
    "t is the maximum allowed value for the sum of the absolute values of the coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca5b7f-8bac-4f16-b972-47439bdd5e2f",
   "metadata": {},
   "source": [
    "Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load dataset\n",
    "data = pd.read_csv('https://raw.githubusercontent.com/scikit-learn/scikit-learn/master/sklearn/datasets/data/boston_house_prices.csv')\n",
    "\n",
    "# Split data into predictors (X) and response variable (y)\n",
    "X = data.drop('MEDV', axis=1)\n",
    "y = data['MEDV']\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Initialize Lasso regression model\n",
    "lasso = Lasso(alpha=0.1)\n",
    "\n",
    "# Fit the model on the training data\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = lasso.predict(X_test)\n",
    "\n",
    "# Calculate the mean squared error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "\n",
    "# Print the coefficients of the model\n",
    "coefficients = pd.DataFrame({'Features': X_train.columns, 'Coefficients': lasso.coef_})\n",
    "print(coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7038b6e-9d7b-4897-a9db-dca2ff90fd2a",
   "metadata": {},
   "source": [
    "##Output\n",
    "Mean Squared Error: 27.08595667528009\n",
    "    Features  Coefficients\n",
    "0       CRIM     -0.117790\n",
    "1         ZN      0.044201\n",
    "2      INDUS      0.001439\n",
    "3       CHAS      2.439364\n",
    "4        NOX    -16.632210\n",
    "5         RM      3.844710\n",
    "6        AGE      0.011159\n",
    "7        DIS     -1.391767\n",
    "8        RAD      0.243762\n",
    "9        TAX     -0.011868\n",
    "10   PTRATIO     -0.962174\n",
    "11         B      0.009450\n",
    "12     LSTAT     -0.541900\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e38c558-3432-49c4-b710-26dbdb809d26",
   "metadata": {},
   "source": [
    "Role of alpha\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alphas = np.linspace(0.01,500,100)\n",
    "lasso = Lasso(max_iter=10000)\n",
    "coefs = []\n",
    "\n",
    "for a in alphas:\n",
    "    lasso.set_params(alpha=a)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    coefs.append(lasso.coef_)\n",
    "\n",
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "plt.axis('tight')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('Standardized Coefficients')\n",
    "plt.title('Lasso coefficients as a function of alpha');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b229d143-cbc2-4764-85d2-67dcabb1c498",
   "metadata": {},
   "source": [
    "## Q6. What is the difference between Ridge Regression and Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039b8f36-a60c-40b7-b76d-636b2f8132cb",
   "metadata": {},
   "source": [
    "Ans-Ridge Regression :\n",
    "In Ridge regression, we add a penalty term which is equal to the square of the coefficient. The L2 term is equal to the square of the magnitude of the coefficients. We also add a coefficient  \\lambda  to control that penalty term. In this case if  \\lambda  is zero then the equation is the basic OLS else if  \\lambda \\, > \\, 0 then it will add a constraint to the coefficient. As we increase the value of \\lambda this constraint causes the value of the coefficient to tend towards zero. This leads to tradeoff of higher bias (dependencies on certain coefficients tend to be 0 and on certain coefficients tend to be very large, making the model less flexible) for lower variance.flexible) for lower variance.\n",
    "\n",
    "\n",
    " L_{ridge} = argmin_{\\hat{\\beta}}\\left ({\\left \\| Y-  \\beta * X \\right \\|}^{2} + \\lambda * {\\left \\| \\beta \\right \\|}_{2}^{2}  \\right ) \n",
    "\n",
    "where \\lambda is regularization penalty.\n",
    "Limitation of Ridge Regression: Ridge regression decreases the complexity of a model but does not reduce the number of variables since it never leads to a coefficient been zero rather only minimizes it. Hence, this model is not good for feature reduction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff716aa4-b2ff-497d-9a2c-efb338da37a3",
   "metadata": {},
   "source": [
    "Lasso Regression :\n",
    "Lasso regression stands for Least Absolute Shrinkage and Selection Operator. It adds penalty term to the cost function. This term is the absolute sum of the coefficients. As the value of coefficients increases from 0 this term penalizes, cause model, to decrease the value of coefficients in order to reduce loss. The difference between ridge and lasso regression is that it tends to make coefficients to absolute zero as compared to Ridge which never sets the value of coefficient to absolute zero.\n",
    "\n",
    "\n",
    " L_{lasso} = argmin_{\\hat{\\beta}}\\left ({\\left \\| Y- \\beta * X \\right \\|}^{2} + \\lambda * {\\left \\| \\beta  \\right \\|}_{1}  \\right ) \n",
    "\n",
    "Limitation of Lasso Regression:\n",
    "Lasso sometimes struggles with some types of data. If the number of predictors (p) is greater than the number of observations (n), Lasso will pick at most n predictors as non-zero, even if all predictors are relevant (or may be used in the test set).\n",
    "If there are two or more highly collinear variables then LASSO regression select one of them randomly which is not good for the interpretation of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57e8bd2-73fa-4c4b-8ef6-7394c9cc85af",
   "metadata": {},
   "source": [
    "## Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef71f8a3-dbda-4472-9810-4d8bedb79457",
   "metadata": {},
   "source": [
    "LASSO stands for Least Absolute Shrinkage and Selection Operator. I know it doesn’t give much of an idea, but there are 2 keywords here – ‘absolute‘ and ‘selection. ‘\n",
    "\n",
    "Let’s consider the former first and worry about the latter later.\n",
    "\n",
    "Lasso regression performs L1 regularization, i.e., it adds a factor of the sum of the absolute value of coefficients in the optimization objective. Thus, lasso regression optimizes the following:\n",
    "\n",
    "Objective = RSS + α * (sum of the absolute value of coefficients)\n",
    "\n",
    "Here, α (alpha) works similar to that of the ridge and provides a trade-off between balancing RSS and the magnitude of coefficients. Like that of the ridge, α can take various values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0bdced2-364d-4b90-a36b-cf01da085638",
   "metadata": {},
   "source": [
    "α = 0: Same coefficients as simple linear regression\n",
    "α = ∞: All coefficients zero (same logic as before)\n",
    "0 < α < ∞: coefficients between 0 and that of simple linear regression\n",
    "Yes, its appearing to be very similar to Ridge till now. But hang on with me, and you’ll know the difference by the time we finish. Like before, let’s run lasso regression on the same problem as above. First, we’ll define a generic function:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9af5e35-5fbd-4044-9e32-0118775340ae",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "def lasso_regression(data, predictors, alpha, models_to_plot={}):\n",
    "    #Fit the model\n",
    "    lassoreg = Lasso(alpha=alpha,normalize=True, max_iter=1e5)\n",
    "    lassoreg.fit(data[predictors],data['y'])\n",
    "    y_pred = lassoreg.predict(data[predictors])\n",
    "    \n",
    "    #Check if a plot is to be made for the entered alpha\n",
    "    if alpha in models_to_plot:\n",
    "        plt.subplot(models_to_plot[alpha])\n",
    "        plt.tight_layout()\n",
    "        plt.plot(data['x'],y_pred)\n",
    "        plt.plot(data['x'],data['y'],'.')\n",
    "        plt.title('Plot for alpha: %.3g'%alpha)\n",
    "    \n",
    "    #Return the result in pre-defined format\n",
    "    rss = sum((y_pred-data['y'])**2)\n",
    "    ret = [rss]\n",
    "    ret.extend([lassoreg.intercept_])\n",
    "    ret.extend(lassoreg.coef_)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ebe29-602f-4aa0-9efa-71fbe020ff92",
   "metadata": {},
   "source": [
    "Multicollinearity refers to the condition when two or more independent features are correlated to each other. The change in one of the collinear features may affect the other related features. Multicollinearity in the dataset may be caused due to poor designing of experiments while collecting the data or maybe introduced while creating new features.\n",
    "\n",
    "Multicollinearity may cause to make the coefficients unstable after training a regression model. The presence of the correlated features may not add any new valuable information to the model.\n",
    "\n",
    "The condition of multicollinearity needs to be detected and handled prior to modeling the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95307179-80da-472f-83b8-841a20bdb3a9",
   "metadata": {},
   "source": [
    "Lasso regression is a linear regression technique with L1 prior as a regularize. The idea is to reduce the multicollinearity by regularization by reducing the coefficients of the feature that are multicollinear.\n",
    "\n",
    "By increasing the alpha value for the L1 regularizer, we introduce some small bias in the estimator that breaks the correlation and reduces the variance.\n",
    "\n",
    "Scikit-learn package offers API to perform Lasso Regression in a single line of Python code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb708e-e1d4-4443-9aa8-b94115118922",
   "metadata": {},
   "source": [
    "## Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f551db-b873-4715-b87a-9a0a058eb19c",
   "metadata": {},
   "source": [
    "Model developers tune the overall impact of the regularization term by multiplying its value by a scalar known as lambda (also called the regularization rate). That is, model developers aim to do the following:\n",
    "\n",
    "Performing L2 regularization has the following effect on a model\n",
    "\n",
    "Encourages weight values toward 0 (but not exactly 0)\n",
    "Encourages the mean of the weights toward 0, with a normal (bell-shaped or Gaussian) distribution.\n",
    "Increasing the lambda value strengthens the regularization effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b152425-a364-4a24-9513-cc408bb2db2a",
   "metadata": {},
   "source": [
    "Selecting a good value for λ is critical. When λ=0, the penalty term has no effect, and ridge regression will produce the classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and the ridge regression coefficients will get close zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3712e2b-bcca-4084-be28-b0e2683701db",
   "metadata": {},
   "source": [
    "When choosing a lambda value, the goal is to strike the right balance between simplicity and training-data fit: If your lambda value is too high, your model will be simple, but you run the risk of underfitting your data. Your model won't learn enough about the training data to make useful predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa53758-1cc4-4f81-954e-9ee77781d87b",
   "metadata": {},
   "source": [
    "For lasso regression, the alpha value is 1. The output is the best cross-validated lambda, which comes out to be 0.001."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9ffacd-36c1-442d-bbb8-bd431598426c",
   "metadata": {},
   "source": [
    "Lasso regression, or the Least Absolute Shrinkage and Selection Operator, is also a modification of linear regression. In lasso, the loss function is modified to minimize the complexity of the model by limiting the sum of the absolute values of the model coefficients (also called the l1-norm).\n",
    "\n",
    "The loss function for lasso regression can be expressed as below:\n",
    "\n",
    "Loss function = OLS + alpha * summation (absolute values of the magnitude of the coefficients)\n",
    "\n",
    "In the above function, alpha is the penalty parameter we need to select. Using an l1-norm constraint forces some weight values to zero to allow other coefficients to take non-zero values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d621c687-fc6b-469d-8940-56497a0dcba0",
   "metadata": {},
   "source": [
    "The first step to build a lasso model is to find the optimal lambda value using the code below. For lasso regression, the alpha value is 1. The output is the best cross-validated lambda, which comes out to be 0.001.\n",
    "lambdas <- 10^seq(2, -3, by = -.1)\n",
    "\n",
    "# Setting alpha = 1 implements lasso regression\n",
    "lasso_reg <- cv.glmnet(x, y_train, alpha = 1, lambda = lambdas, standardize = TRUE, nfolds = 5)\n",
    "\n",
    "# Best \n",
    "lambda_best <- lasso_reg$lambda.min \n",
    "lambda_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2275e4f9-83ca-48a4-98a6-45e658dabd0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
