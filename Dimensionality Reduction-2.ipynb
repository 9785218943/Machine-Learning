{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf31f3a9-0de1-4bb5-86b2-d45d434b3313",
   "metadata": {},
   "source": [
    "## Q1. What is a projection and how is it used in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e64e4-c620-4b48-9d3a-0967608544ae",
   "metadata": {},
   "source": [
    "Ans-\n",
    "Principal Component Analysis, or PCA for short, is a method for reducing the dimensionality of data. It can be thought of as a projection method where data with m-columns (features) is projected into a subspace with m or fewer columns, whilst retaining the essence of the original data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c5adf2-7123-4af3-8f5d-0f629912a122",
   "metadata": {},
   "source": [
    "it used in PCA:\n",
    "\n",
    "split the data set into training set and test set.\n",
    "apply logistic regression to the training set.\n",
    "make prediction on test set.\n",
    "output the prediction score on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be473788-66ac-4fb2-b5ee-b3463b1c6b27",
   "metadata": {},
   "source": [
    "## Q2. How does the optimization problem in PCA work, and what is it trying to achieve?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568168a7-dec9-4f3a-99cd-5509c65bb73a",
   "metadata": {},
   "source": [
    "Ans-Principal component analysis (PCA) is one of the most widely used multivariate techniques in statistics. It is commonly used to reduce the dimensionality of data in order to examine its underlying structure and the covariance/correlation structure of a set of variables.\n",
    "\n",
    "Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb9b4d3-7e15-4dfd-9547-adbe441cde7e",
   "metadata": {},
   "source": [
    "## Q3. What is the relationship between covariance matrices and PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71da54be-9a93-423c-96bb-5d32cfd3ea28",
   "metadata": {},
   "source": [
    "Ans-PCA is simply described as “diagonalizing the covariance matrix”. What does diagonalizing a matrix mean in this context? It simply means that we need to find a non-trivial linear combination of our original variables such that the covariance matrix is diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217b7c1-d3a6-4c1e-ab1d-178c5529e7f9",
   "metadata": {},
   "source": [
    "As stated earlier, the covariance matrix will explain the relationship between any two features in the data. This process is used in collinear variables. A positive covariance value shows a direct relationship (both variables increase or decrease)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a617b2-c8f0-4655-93fe-f42b49261e8a",
   "metadata": {},
   "source": [
    "## Q4. How does the choice of number of principal components impact the performance of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7a64708-20e0-4e55-a2e2-37264905d557",
   "metadata": {},
   "source": [
    "Ans-If our sole intention of doing PCA is for data visualization, the best number of components is 2 or 3. If we really want to reduce the size of the dataset, the best number of principal components is much less than the number of variables in the original dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da61bac-bf5d-413d-a397-8e56c9b5d473",
   "metadata": {},
   "source": [
    "## Q5. How can PCA be used in feature selection, and what are the benefits of using it for this purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0fe8aa-61e4-405d-a339-abbaabd2ef9e",
   "metadata": {},
   "source": [
    "Ans-Principal Component Analysis (PCA) can help you reach various objectives, such as decreasing the number of variables and avoiding the curse of dimensionality, which can make data easier to manage, store, and analyze. It can also eliminate multicollinearity and noise, improving the accuracy and stability of statistical or machine learning models. Moreover, PCA can extract latent features and discover hidden patterns, enhancing your understanding of the data and generating new insights. Additionally, it can visualize high-dimensional data in lower dimensions, aiding in exploring the data and communicating the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a0ebf5-4cba-4857-b936-d64e3a07bb54",
   "metadata": {},
   "source": [
    "The performance of machine learning model is directly proportional to the data features used to train it. The performance of ML model will be affected negatively if the data features provided to it are irrelevant. On the other hand, use of relevant data features can increase the accuracy of your ML model especially linear and logistic regression.\n",
    "\n",
    "Now the question arise that what is automatic feature selection? It may be defined as the process with the help of which we select those features in our data that are most relevant to the output or prediction variable in which we are interested. It is also called attribute selection.\n",
    "\n",
    "The following are some of the benefits of automatic feature selection before modeling the data −\n",
    "\n",
    "Performing feature selection before data modeling will reduce the overfitting.\n",
    "\n",
    "Performing feature selection before data modeling will increases the accuracy of ML model.\n",
    "\n",
    "Performing feature selection before data modeling will reduce the training time\n",
    "\n",
    "Feature "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3edc63e4-a429-40d4-9eb3-8a239b6e46cc",
   "metadata": {},
   "source": [
    "from pandas import read_csv\n",
    "from numpy import set_printoptions\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "path = r'C:\\pima-indians-diabetes.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "dataframe = read_csv(path, names=names)\n",
    "array = dataframe.values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f28280-d250-47fe-85c7-c0db845dc0e7",
   "metadata": {},
   "source": [
    "## Q6. What are some common applications of PCA in data science and machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5e391-bb59-42bd-883a-3899768c6f1c",
   "metadata": {},
   "source": [
    "ANS-The Principal Components are a straight line that captures most of the variance of the data. They have a direction and magnitude. Principal components are orthogonal projections (perpendicular) of data onto lower-dimensional space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63808c00-62e3-45a3-b08a-6ba18ac58398",
   "metadata": {},
   "source": [
    "Application of PCA in Machine Learning:\n",
    "    \n",
    "1.PCA is used to visualize multidimensional data.\n",
    "\n",
    "2.It is used to reduce the number of dimensions in healthcare data.\n",
    "\n",
    "3.PCA can help resize an image.\n",
    "\n",
    "4.It can be used in finance to analyze stock data and forecast returns.\n",
    "\n",
    "5.PCA helps to find patterns in the high-dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7cc0a-9627-4822-b19a-19e6269c3c57",
   "metadata": {},
   "source": [
    "## Q7.What is the relationship between spread and variance in PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafb2605-ae81-4a13-8bf6-945a2d92ac94",
   "metadata": {},
   "source": [
    "Ans-Variance: Variance is the spread of the data in a dataset. In PCA, the variables are transformed in such a way that they explain variance of the dataset in decreasing manner. Co-variance: Covariance provides a measure of the strength of the correlation between two or more sets of random variates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b29993-2b40-4d1c-8052-65d13026089b",
   "metadata": {},
   "source": [
    "## Q8. How does PCA use the spread and variance of the data to identify principal components?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98433a8-d97a-46d7-930b-30d897e72c56",
   "metadata": {},
   "source": [
    "Ans-PCA works by finding the directions of maximum variance in the data set and projecting the data onto these directions. The principal components are ordered by the amount of variance they explain and are used for feature selection, data compression, clustering, and classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81035b21-d020-4cbd-9df5-a9c118980353",
   "metadata": {},
   "source": [
    "## Q9. How does PCA handle data with high variance in some dimensions but low variance in others?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9981a9-530d-4d7e-b69f-69eff5c6fe76",
   "metadata": {},
   "source": [
    "Ans-\n",
    "Standardize the range of continuous initial variables\n",
    "\n",
    "Compute the covariance matrix to identify correlations\n",
    "\n",
    "Compute the eigenvectors and eigenvalues of the covariance matrix to identify the principal components\n",
    "\n",
    "Create a feature vector to decide which principal components to keep.\n",
    "\n",
    "Recast the data along the principal components axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b72ffa2-2772-423a-8eb2-1a2cee9d13c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8581d790-fca1-4420-bdb1-589ddcedd1f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f54384-5f29-4aef-9d9e-eead2bb364e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90118e8-bbaf-4f3a-a045-42459d601a45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9b77cd-3079-4c6a-8049-bf70710b85b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
