{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "596c001a-b343-40a8-851c-44674529a563",
   "metadata": {},
   "source": [
    "## Q1- Define  overfitting and underfitting in machine learning.What are the consequences of each,and how can they be mitigated ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4427f7-6c40-4c7a-88ca-bf542cb48fcf",
   "metadata": {},
   "source": [
    "Ans- Model performance  a model is said to be a good machine learning model if it generalizes any new input data from  problem domain in a proper way.This helps us to make predictions about future data,that tha data model never seen.Now, suppose we want to check how well our machine learning model learns and generalizes to the new data.we have overfitting and underfitting,which are majorly responsible for the poor performanced of the machine learning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a29fc96-c52a-450e-9e9b-26f254363224",
   "metadata": {},
   "source": [
    "Underfitting means that your model makes accurate,but initially incorrect predictions.In case,train error is large and validation/test error is large too.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40353404-78cf-4142-ac09-84b2431663ce",
   "metadata": {},
   "source": [
    "## Reasons for Underfitting:\n",
    "\n",
    "1.High bias and low variance.\n",
    "\n",
    "2.The size of the training dataset used is not enough.\n",
    "\n",
    "3.The model is too simple. \n",
    "\n",
    "4.Training data is not cleaned and also contains noise in it.\n",
    "\n",
    "## Techniques to reduce underfitting: \n",
    "\n",
    "1.Increase model complexity\n",
    "\n",
    "2.Increase the number of features, performing feature engineering\n",
    "\n",
    "3.Remove noise from the data.\n",
    "\n",
    "4.Increase the number of epochs or increase the duration of training to get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d48d62-90c0-4ce1-ab83-9232940f39de",
   "metadata": {},
   "source": [
    "Overfitting means that your model makes not accurate predictions,In this case,train error is very small and validation/test error is large."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9aa4ca-4082-437b-860b-ba7ec8e6089f",
   "metadata": {},
   "source": [
    "## Reasons for Overfitting are as follows:\n",
    " 1.High variance and low bias \n",
    " \n",
    "2.The model is too complex\n",
    "\n",
    "3.The size of the training data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4559bf91-0c02-45e1-af79-d75dbf457377",
   "metadata": {},
   "source": [
    "## Techniques to reduce overfitting:\n",
    "\n",
    "1.Increase training data.\n",
    "\n",
    "2.Reduce model complexity.\n",
    "\n",
    "3.Early stopping during the training phase (have an eye over the loss over the training period as soon as loss begins to increase stop training).\n",
    "\n",
    "4.Ridge Regularization and Lasso Regularization\n",
    "\n",
    "5.Use dropout for neural networks to tackle overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "688447de-0421-40e9-a6f6-a39aea3c24d0",
   "metadata": {},
   "source": [
    "## Q2.How can we reduce overfitting ? Explain in brief."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb0b7e4-0654-459c-b3e3-13c2a4cbdd99",
   "metadata": {},
   "source": [
    "Ans-Reduce overfitting\n",
    "# Train with more data\n",
    "It won’t work every time, but training with more data can help algorithms detect the signal better. In the earlier example of modeling height vs. age in children, it’s clear how sampling more schools will help your model.\n",
    "\n",
    "Of course, that’s not always the case. If we just add more noisy data, this technique won’t help. That’s why you should always ensure your data is clean and relevant.\n",
    "# Remove features\n",
    "Some algorithms have built-in feature selection.\n",
    "\n",
    "For those that don’t, you can manually improve their generalizability by removing irrelevant input features.\n",
    "\n",
    "An interesting way to do so is to tell a story about how each feature fits into the model. This is like the data scientist’s spin on software engineer’s rubber duck debugging technique, where they debug their code by explaining it, line-by-line, to a rubber duck.\n",
    "\n",
    "If anything doesn’t make sense, or if it’s hard to justify certain features, this is a good way to identify them.\n",
    "In addition, there are several feature selection heuristics you can use for a good starting point.\n",
    "# Early stopping\n",
    "When you’re training a learning algorithm iteratively, you can measure how well each iteration of the model performs.\n",
    "\n",
    "Up until a certain number of iterations, new iterations improve the model. After that point, however, the model’s ability to generalize can weaken as it begins to overfit the training data.\n",
    "\n",
    "Early stopping refers stopping the training process before the learner passes that point.\n",
    "\n",
    "early stopping in machine learning\n",
    "Today, this technique is mostly used in deep learning while other techniques (e.g. regularization) are preferred for classical machine learning.\n",
    "# Regularization\n",
    "Regularization refers to a broad range of techniques for artificially forcing your model to be simpler.\n",
    "\n",
    "The method will depend on the type of learner you’re using. For example, you could prune a decision tree, use dropout on a neural network, or add a penalty parameter to the cost function in regression.\n",
    "\n",
    "Oftentimes, the regularization method is a hyperparameter as well, which means it can be tuned through cross-validation.\n",
    "\n",
    "We have a more detailed discussion here on algorithms and regularization methods.\n",
    "# Ensembling\n",
    "Ensembles are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the two most common are:\n",
    "\n",
    "\n",
    "Bagging attempts to reduce the chance overfitting complex models.\n",
    "\n",
    "It trains a large number of “strong” learners in parallel.\n",
    "A strong learner is a model that’s relatively unconstrained.\n",
    "Bagging then combines all the strong learners together in order to “smooth out” their predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42606e0b-f870-4f00-8edf-2b582a64b41f",
   "metadata": {},
   "source": [
    "## Q5 Discuss some common methods for detecting overfitting and underfitting in machine learning models.\n",
    "How can you determine whether your model is overfitting or underfitting? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7473b9-d2d4-42bf-8a3a-ee9644368f02",
   "metadata": {},
   "source": [
    "Ans-A key challenge with overfitting, and with machine learning in general, is that we can’t know how well our model will perform on new data until we actually test it.\n",
    "\n",
    "To address this, we can split our initial dataset into separate training and test subsets.\n",
    " our model does much better on the training set than on the test set, then we’re likely overfitting.\n",
    "\n",
    "For example, it would be a big red flag if our model saw 99% accuracy on the training set but only 55% accuracy on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b75074-a46f-4d97-b95f-c7869bf414ff",
   "metadata": {},
   "source": [
    "We can determine whether a predictive model is underfitting or overfitting the training data by looking at the prediction error on the training data and the evaluation data. Your model is underfitting the training data when the model performs poorly on the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc538e40-3eff-4e0c-9961-de4f338733b6",
   "metadata": {},
   "source": [
    "## Q6Compare and contrast bias and variance in machine learning. What are some examples of high bias\n",
    "# and high variance models, and how do they differ in terms of their performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016cc7f1-d1da-4e28-a604-b224aa7d0996",
   "metadata": {},
   "source": [
    "# Ans-Bias\n",
    "When an algorithm is employed in a machine learning model and it does not fit well, a phenomenon known as bias can develop. Bias arises in several situations.\n",
    "\n",
    "The disparity between the values that were predicted and the values that were actually observed is referred to as bias.\n",
    "\n",
    "The model is incapable of locating patterns in the dataset that it was trained on, and it produces inaccurate results for both seen and unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac6a1bc-2a3a-4bb3-b83e-355ede398303",
   "metadata": {},
   "source": [
    "## Variance\n",
    "The term \"variance\" refers to the degree of change that may be expected in the estimation of the target function as a result of using multiple sets of training data\n",
    "\n",
    "A random variable's variance is a measure of how much it varies from the value that was predicted for it.\n",
    "\n",
    "The model recognizes the majority of the dataset's patterns and can even learn from the noise or data that isn't vital to its operation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6f4851-c17e-4d5b-9d3f-74a1676029e4",
   "metadata": {},
   "source": [
    "## Q7- What is regularization in machine learning, and how can it be used to prevent overfitting? Describe\n",
    "## some common regularization techniques and how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f550f601-0a5a-4b31-b6c6-0ec9add3c45f",
   "metadata": {},
   "source": [
    "Ans- REGULARIZATION in machine learning is the process of regularizing the parameters that constrain,regularizes,or shrinks the coefficient estimates towards zero,in other words,this technique discourage learning a more complex or flexible model,avoiding the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5906c50d-1ce0-4fac-8619-aa114432f1b7",
   "metadata": {},
   "source": [
    "Train with more data. ...\n",
    "Data augmentation. ...\n",
    "Addition of noise to the input data. ...\n",
    "Feature selection. ...\n",
    "Cross-validation. ...\n",
    "Simplify data. ...\n",
    "Regularization. ...\n",
    "Ensembling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36faa96-5cdb-4df9-9b75-13a96a765813",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c9d943-a3a5-41d9-8950-30011679077f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09314bfc-9e11-4008-af77-a602f2b4174a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
