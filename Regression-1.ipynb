{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d22e57-e965-4662-bec3-32e312fb1bd9",
   "metadata": {},
   "source": [
    "## Q1. Explain the difference between simple linear regression and multiple linear regression. Provide an\n",
    "# example of each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff739fc-1292-4e20-af33-8bb741549c66",
   "metadata": {},
   "source": [
    "AnsUse of Regression Analysis\n",
    "Before starting lets quickly understand the use of regression analysis .Regression analysis is primarily used for two conceptually distinct purposes.\n",
    "\n",
    "1.Regression analysis is widely used for prediction and forecasting, where its use has substantial overlap with the field of machine learning.\n",
    "\n",
    "2.In some situations regression analysis can be used to infer causal relationships between the independent and dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12f7262-1183-41d7-8ddd-4eb3aa411840",
   "metadata": {},
   "source": [
    "What Is Linear Regression?\n",
    "Linear Regression is a supervised learning predictive modeling algorithm in machine learning. The model predicte value according to independent variables and helps in finding the relationship between those variables.\n",
    "\n",
    "Types of Regression:\n",
    "\n",
    "1.Simple Linear Regression: Simple Linear Regression is the model that estimates relationship between one independent variable and one dependent variable or target variable using a straight line.\n",
    "\n",
    "2.Multiple Linear Regression: Multiple linear regression is a model that is used to analyze the relationship between two or more independent variables and single dependent variable or target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6561b28-ef86-4d1c-94ed-a949536e16aa",
   "metadata": {},
   "source": [
    "\n",
    "Linear Regression is a supervised learning predictive modeling algorithm in machine learning. The model predicte value according to independent variables and helps in finding the relationship between those variables.\n",
    "\n",
    "Types of Regression:\n",
    "\n",
    "1.Simple Linear Regression: Simple Linear Regression is the model that estimates relationship between one independent variable and one dependent variable or target variable using a straight line.\n",
    "\n",
    "2.Multiple Linear Regression: Multiple linear regression is a model that is used to analyze the relationship between two or more independent variables and single dependent variable or target variable.\n",
    "\n",
    "Steps of Linear Regression\n",
    "As the name suggested, the idea behind performing Linear Regression(simple linear regression) is that we should come up with a linear equation that describes the relationship between dependent and independent variables .\n",
    "\n",
    "Step 1\n",
    "Let’s assume that we have a dataset where x is the independent variable and Y is a function of x (Y=f(x)). Thus, by using Linear Regression we can form the following equation (equation for the best-fitted line):\n",
    "\n",
    "      Y = mx + c\n",
    "\n",
    "y denotes response variable\n",
    "\n",
    "x denotes i’th predictor variable\n",
    "\n",
    "This is an equation of a straight line where m is the slope of the line and c is the intercept.\n",
    "\n",
    "Step 2\n",
    "Now, to derive the best-fitted line, first, we assign random values to m and c and calculate the corresponding value of the given training data points Y for a given x. This Y value is the output value.\n",
    "\n",
    "Step 3\n",
    "Now, as we have our calculated output value (let’s represent it as ŷ), we can verify whether our prediction is accurate or not. In the case of Linear Regression, we calculate this error (residual) by using the MSE method (mean squared error) and we name it as loss function:          \n",
    "             L = 1/n ∑((y – ŷ)2)\n",
    "             \n",
    "Where n is the number of observations.\n",
    "\n",
    "Step 4\n",
    "To achieve the best-fitted line, we have to minimize the value of the loss function. To minimize the loss function, we use a technique called gradient descent. Let’s discuss how gradient descent works (although I wi             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f5f69f-2a6e-489f-bec1-312d8937a416",
   "metadata": {},
   "source": [
    "## Simple and Multiple Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6294ec2f-1415-40a9-b2b1-253a895b772c",
   "metadata": {},
   "source": [
    "There is just one x and one y variable in simple linear regression.\n",
    "\n",
    "There is one y variable and two or more x variables in multiple linear regression.\n",
    "\n",
    "To understand each concept clearly, the first thing to do is to discuss linear regression assumptions and state a linear regression example to make an already theoretical idea much more grounded.\n",
    "\n",
    "A simple linear regression model usually takes the form of:\n",
    "\n",
    " .\n",
    "\n",
    "Considering the above-stated formula, there are a couple of assumptions or requirements that must be met for a formula to be regarded as a simple linear regression, and they are;\n",
    "\n",
    "Linear relationship: The independent variable, x, and the dependent variable, y, have a linear relationship.\n",
    "Independent residuals: The residuals are self-contained. In time series results, there is no connection between consecutive residuals in particular."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b199e9b-c59f-4cd8-bf39-9fbf4b0b81f8",
   "metadata": {},
   "source": [
    "Homoscedasticity: At any degree of x, the residuals have the same variance.\n",
    "Normality: The model’s residuals have a regular distribution.\n",
    "If any of these assumptions are broken, any linear regression findings can be inaccurate or even misleading. A typical example of linear regression can be something along the lines of the following;\n",
    "\n",
    "Assume you own chocolate business. A simple linear regression will entail you determining a connection between revenue and product texture, with revenue as the dependent variable and product texture as the independent variable. \n",
    "\n",
    "With multiple regression models, on the other hand, the equation looks more like this:\n",
    "\n",
    "multiple regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45239dd-6d4d-46a3-a777-c8050d8fa94d",
   "metadata": {},
   "source": [
    " Multiple Linear Regression assumes there is a linear relationship between two or more independent variables and one dependent variable. \n",
    "\n",
    "The Formula for multiple linear regression:\n",
    "\n",
    "     Y=B0+B0X1+B2X2+……+BnXn+e\n",
    "\n",
    "Y =   the predicted value of the dependent variable\n",
    "\n",
    "B0 = the y-intercept (value of y when all other parameters are set to 0)\n",
    "\n",
    "B1X1= the regression coefficient (B1)  of the first independent variable (X1) \n",
    "\n",
    "BnXn = the regression coefficient of the last independent variable\n",
    "\n",
    "e = model error "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2632127e-393a-4f47-ab5a-1a78d147c815",
   "metadata": {},
   "source": [
    "## Q2. Discuss the assumptions of linear regression. How can you check whether these assumptions hold in\n",
    "# a given dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9627d24a-98d3-4867-8986-9ccd7ac4a130",
   "metadata": {},
   "source": [
    "Ans-Regression analysis marks the first step in predictive modeling. No doubt, it’s fairly easy to implement. Neither it’s syntax nor its parameters create any kind of confusion. But, merely running just one line of code, doesn’t solve the purpose. Neither just looking at R² or MSE values. Regression tells much more than that.\n",
    "In R, regression analysis return 4 plots using plot(model_name) function. Each of the plot provides significant information or rather an interesting story about the data. Sadly, many of the beginners either fail to decipher the information or don’t care about what these plots say. Once you understand these plots, you’d be able to bring significant improvement in your regression model.\n",
    "\n",
    "For model improvement, you also need to understand regression assumptions and ways to fix them when they get violated.\n",
    "\n",
    "     Assumptions in Regression\n",
    "Regression is a parametric approach. ‘Parametric’ means it makes assumptions about data for the purpose of analysis. Due to its parametric side, regression is restrictive in nature. It fails to deliver good results with data sets which doesn’t fulfill its assumptions. Therefore, for a successful regression analysis, it’s essential to validate these assumptions.\n",
    "\n",
    "So, how would you check (validate) if a data set follows all regression assumptions? You check it using the regression plots (explained below) along with some statistical test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c339b7-6ae8-4a25-811d-2b61630e012c",
   "metadata": {},
   "source": [
    "Let’s look at the important assumptions in regression analysis:\n",
    "\n",
    "There should be a linear and additive relationship between dependent (response) variable and independent (predictor) variable(s). A linear relationship suggests that a change in response Y due to one unit change in X¹ is constant, regardless of the value of X¹. An additive relationship suggests that the effect of X¹ on Y is independent of other variables.\n",
    "\n",
    "There should be no correlation between the residual (error) terms. Absence of this phenomenon is known as Autocorrelation.\n",
    "\n",
    "The independent variables should not be correlated. Absence of this phenomenon is known as multicollinearity.The error terms must have constant variance. \n",
    "This phenomenon is known as homoskedasticity. The presence of non-constant variance is referred to heteroskedasticity.\n",
    "\n",
    " The error terms must be normally distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e09082-7c33-45aa-98fa-93f2236a6b6d",
   "metadata": {},
   "source": [
    "    What if these assumptions get violated ?\n",
    "Let’s dive into specific assumptions and learn about their outcomes (if violated):\n",
    "\n",
    "1. Linear and Additive:  If you fit a linear model to a non-linear, non-additive data set, the regression algorithm would fail to capture the trend mathematically, thus resulting in an inefficient model. Also, this will result in erroneous predictions on an unseen data set.\n",
    "\n",
    "How to check: Look for residual vs fitted value plots (explained below). Also, you can include polynomial terms (X, X², X³) in your model to capture the non-linear effect.\n",
    "\n",
    "Autocorrelation: The presence of correlation in error terms drastically reduces model’s accuracy. This usually occurs in time series models where the next instant is dependent on previous instant. If the error terms are correlated, the estimated standard errors tend to underestimate the true standard error.\n",
    "\n",
    "If this happens, it causes confidence intervals and prediction intervals to be narrower. Narrower confidence interval means that a 95% confidence interval would have lesser probability than 0.95 that it would contain the actual value of coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28888fb-d671-467a-a204-b5738d5b7592",
   "metadata": {},
   "source": [
    "## Q3. How do you interpret the slope and intercept in a linear regression model? Provide an example using\n",
    "# a real-world scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0ffd17-6f99-4cd5-ba4f-08022b8959f4",
   "metadata": {},
   "source": [
    "Ans-Interpreting the Intercept in a regression model isn’t always as straightforward as it looks.\n",
    "\n",
    "Here’s the definition: the intercept (often labeled the constant) is the expected value of Y when all X=0. But that definition isn’t always helpful. So what does it really mean.\n",
    "\n",
    "             Regression with One Predictor X\n",
    "             \n",
    "Start with a very simple regression equation, with one predictor, X.\n",
    "\n",
    "If X sometimes equals 0, the intercept is simply the expected value of Y at that value. In other words, it’s the mean of Y at one value of X. That’s meaningful. \n",
    "If X never equals 0, then the intercept has no intrinsic meaning. You literally can’t interpret it. That’s actually fine, though. You still need that intercept to give you unbiased estimates of the slope and to calculate accurate predicted values. So while the intercept has a purpose, it’s not meaningful.\n",
    "\n",
    "Both these scenarios are common in real data.In scientific research, the purpose of a regression model is one of two things.\n",
    "One is to understand the relationship between predictors and the response.  If so, and if X never = 0, there is no interest in the intercept. It doesn’t tell you anything about the relationship between X and Y.\n",
    "\n",
    "So whether the value of the intercept is meaningful or not, many times you’re just not interested in it. It’s not answering an actual research question.\n",
    "\n",
    "The other purpose is prediction. You do need the intercept to calculate predicted values.  In market research or data science, there is usually more interest in prediction, so the intercept is more important here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c690c978-5413-43fb-a872-237fe463b652",
   "metadata": {},
   "source": [
    "# When A Meaningful Intercept is Important"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5836d67-7cb7-4be3-bd17-07094d8697a8",
   "metadata": {},
   "source": [
    "When X never equals 0, but you want a meaningful intercept, it’s not hard to adjust things to get a meaningful intercept. Simply consider centering X.\n",
    "\n",
    "Centering sounds fancy, but it’s not. It means to re-scale X so that the mean or some other meaningful value = 0. And all you do to get thats create a new version of X where you just subtract a constant from X.\n",
    "Let’s say X is Age and the mean of Age in your sample 20.\n",
    "\n",
    "It will look something like: NewX = X – 20.\n",
    "\n",
    "That’s it.\n",
    "\n",
    "Just use NewX in your model instead of X. Now the intercept has a meaning. It’s the mean value of Y at the mean value of X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5158cf-664f-4b07-88df-f052260baba1",
   "metadata": {},
   "source": [
    "# Interpreting the Intercept in Regression Models with Multiple Xs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180796f0-481d-4f34-b028-73b66aec2863",
   "metadata": {},
   "source": [
    "It all gets a little trickier when you have more than one X.\n",
    "\n",
    "The definition still holds: the intercept is the expected value of Y when all X=0.\n",
    "\n",
    "The emphasis here is on ALL.\n",
    "\n",
    "And this is where it gets complicated. If all Xs are numerical, it’s an uncommon (though not unheard of) situation for every X to have values of 0. This is often why you’ll hear that intercepts aren’t important or worth interpreting.\n",
    "\n",
    "But you always have the option to center all numerical Xs to get a meaningful intercept.\n",
    "And when some Xs are categorical, the situation is different. Most of the time, categorical variables are dummy coded. Dummy coded variables have values of 0 for the reference group and 1 for the comparison group. Since the intercept is the expected value of Y when X=0, it is the mean value only for the reference group (when all other X=0). So having dummy-coded categorical variables in your model can give the intercept more meaning.\n",
    "\n",
    "This is especially important to consider when the dummy coded predictor is included in an interaction term.  Say for example that X1 is a continuous variable centered at its mean.  X2 is a dummy coded predictor, and the model contains an interaction term for X1*X2.\n",
    "\n",
    "The B value for the intercept is the mean value of X1 only for the reference group.  The mean value of X1 for the comparison group is the intercept plus the coefficient for X2.\n",
    "It’s hard to give an example because it really depends on how X1 and X2 are coded. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16eb7dba-874d-4a2a-9745-096445a3d5e7",
   "metadata": {},
   "source": [
    "## Q4. Explain the concept of gradient descent. How is it used in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f08ea2-4173-44bb-90c9-75851958d3c7",
   "metadata": {},
   "source": [
    "Ans-Gradient descent is an optimization algorithm that is used to minimize the loss function in a machine learning model. The goal of gradient descent is to find the set of weights (or coefficients) that minimize the loss function. The algorithm works by iteratively adjusting the weights in the direction of the steepest decrease in the loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b99d55-8486-4f39-a965-86d1ce6a9e1f",
   "metadata": {},
   "source": [
    "# How does Gradient Descent Work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c2b08f-7df9-4438-88dd-5fe3efd7d372",
   "metadata": {},
   "source": [
    "The basic idea of gradient descent is to start with an initial set of weights and update them in the direction of the negative gradient of the loss function. The gradient is a vector of partial derivatives that represents the rate of change of the loss function with respect to the weights. By updating the weights in the direction of the negative gradient, the algorithm moves towards a minimum of the loss function.\n",
    "The learning rate is a hyperparameter that determines the size of the step taken in the weight update. A small learning rate results in a slow convergence, while a large learning rate can lead to overshooting the minimum and oscillating around the minimum. It’s important to choose an appropriate learning rate that balances the speed of convergence and the stability of the optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141c7062-c4c1-4e43-b3b8-4321f8feae93",
   "metadata": {},
   "source": [
    "## Q5. Describe the multiple linear regression model. How does it differ from simple linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab7617d-57d4-4b43-b05c-d49758aa5fb9",
   "metadata": {},
   "source": [
    "Ans-Multiple regression is a broader class of regressions that encompasses linear and nonlinear regressions with multiple explanatory variables. Whereas linear regress only has one independent variable impacting the slope of the relationship, multiple regression incorporates multiple independent variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889536fa-5019-4a90-9111-7bd3049bbe39",
   "metadata": {},
   "source": [
    "Linear Regression vs. Multiple Regression Example\n",
    "Consider an analyst who wishes to establish a relationship between the daily change in a company's stock prices and the daily change in trading volume. Using linear regression, the analyst can attempt to determine the relationship between the two variables:\n",
    "\n",
    "Daily Change in Stock Price = (Coefficient)(Daily Change in Trading Volume) + (y-intercept)\n",
    "\n",
    "If the stock price increases $0.10 before any trades occur and increases $0.01 for every share sold, the linear regression outcome is:\n",
    "\n",
    "Daily Change in Stock Price = ($0.01)(Daily Change in Trading Volume) + $0.10\n",
    "\n",
    "However, the analyst realizes there are several other factors to consider including the company's P/E ratio, dividends, and prevailing inflation rate. The analyst can perform multiple regression to determine which—and how strongly—each of these variables impacts the stock price:\n",
    "\n",
    "Daily Change in Stock Price = (Coefficient)(Daily Change in Trading Volume) + (Coefficient)(Company's P/E Ratio) + (Coefficient)(Dividend) + (Coefficient)(Inflation Rate)\n",
    "\n",
    "Is Multiple Linear Regression Better Than Simple Linear Regression.\n",
    "\n",
    "Multiple linear regression is a more specific calculation than simple linear regression. For straight-forward relationships, simple linear regression may easily capture the relationship between the two variables. For more complex relationships requiring more consideration, multiple linear regression is often better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5c2d52-3c35-4ba7-8a21-a35a3c58d18b",
   "metadata": {},
   "source": [
    "## Q6. Explain the concept of multicollinearity in multiple linear regression. How can you detect and\n",
    "# address this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85497fd3-4498-49cc-8d2c-a608fe1e95d2",
   "metadata": {},
   "source": [
    "Ans-Multicollinearity happens when independent variables in the regression model are highly correlated to each other. It makes it hard to interpret of model and also creates an overfitting problem. It is a common assumption that people test before selecting the variables into the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dd142c-b755-4a54-9174-547c053c2462",
   "metadata": {},
   "source": [
    "One method to detect multicollinearity is to calculate the variance inflation factor (VIF) for each independent variable, and a VIF value greater than 1.5 indicates multicollinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db1cf63-44a4-42d6-bb1b-3d41b753d0f6",
   "metadata": {},
   "source": [
    "Seven more ways to detect multicollinearity\n",
    "1. Very high standard errors for regression coefficients\n",
    "When standard errors are orders of magnitude higher than their coefficients, that’s an indicator.\n",
    "\n",
    "2. The overall model is significant, but none of the coefficients are\n",
    "Remember that a p-value for a coefficient tests whether the unique effect of that predictor on Y is zero. If all predictors overlap in what they measure, there is little unique effect, even if the predictors as a group have an effect on Y.\n",
    "\n",
    "3. Large changes in coefficients when adding predictors\n",
    "If the predictors are completely independent of each other, their coefficients won’t change at all when you add or remove one. But the more they overlap, the more drastically their coefficients will change.\n",
    "\n",
    "4. Coefficients have signs opposite what you’d expect from theory\n",
    "Be careful here as you don’t want to disregard an unexpected finding as problematic. Not all effects opposite theory indicate a problem with the model. That said, it could be multicollinearity and warrants taking a second look at other indicators.\n",
    "\n",
    "5. Coefficients on different samples are wildly different\n",
    "If you have a large enough sample, split the sample in half and run the model separately on each half. Wildly different coefficients in the two models could be a sign of multicollinearity.\n",
    "6. High Variance Inflation Factor (VIF) and Low Tolerance\n",
    "These two useful statistics are reciprocals of each other. So either a high VIF or a low tolerance is indicative of multicollinearity. VIF is a direct measure of how much the variance of the coefficient (ie. its standard error) is being inflated due to multicollinearity.\n",
    "\n",
    "7. High Condition Indices\n",
    "Condition indices are a bit strange.  The basic idea is to run a Principal Components Analysis on all predictors. If they have a lot of shared information, the first Principal Component will be much higher than the last. Their ratio, the Condition Index, will be high if multicollinearity is present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e452b6c-33d4-472a-9e5b-581d721099f4",
   "metadata": {},
   "source": [
    "## Q7. Describe the polynomial regression model. How is it different from linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dae7de4-5ad5-4be8-bc2f-4cdcb27b7c0e",
   "metadata": {},
   "source": [
    "The algorithm of linear regression works only when the regression in the data is linear. Polynomial regression can be considered one of the exceptional cases of multiple linear regression models. In other words, it is a linear regression type containing dependent and independent variables, and they both share a curvilinear relationship. A polynomial relationship is fitted in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60242274-f7d2-465a-9b47-a62c7dfe0b19",
   "metadata": {},
   "source": [
    "The difference between linear regression and polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf4598a-489a-4612-94b0-12ec97827553",
   "metadata": {},
   "source": [
    "Polynomial regression with only one variable term is known as linear regression. That said, polynomial regressions with more than one variable term have names. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bf12a2-333c-40d7-a3b6-4796cf9d2f00",
   "metadata": {},
   "source": [
    "Types of Polynomial Regression\n",
    "Since there is no limit to the degree in a polynomial equation, and it can go up to the nth value, numerous kinds of polynomial regression exist. For instance, a quadratic equation, when spoken, generally is used for the second degree of a polynomial equation. This degree, as stated, can up to nth value, and you can derive as many equations as you want to or need. Hence, polynomial regression is usually categorized as mentioned below. \n",
    "\n",
    "Linear, when the degree is 1. \n",
    "Quadratic, the degree of this equation is 2. \n",
    "Cubic with a degree as three continues, based on the degree used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a92ac6-6353-4f02-ab74-ac5d8f58cdac",
   "metadata": {},
   "source": [
    "Equation of the Polynomial Regression Model\n",
    "Any linear equation is a polynomial regression that has a degree of 1. The very common and usual equation used to define the regression is; \n",
    "\n",
    "y = mx+b\n",
    "\n",
    "In this equation, m is the slope, and b is the y-intercept. One can easily write this as \n",
    "\n",
    "f(x) = c0 + c1 x where c1 is the slope and the c0 is the y-intercept. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b7e34f-77e9-4767-b7f9-6028ddb43655",
   "metadata": {},
   "source": [
    "Steps for Polynomial Regression\n",
    "Find the steps below to use polynomial regression in machine learning and make the most of it. \n",
    "\n",
    "Step 1: At this step, you need to import the libraries and datasets that will.be used to perform polynomial regression. \n",
    "\n",
    "Step 2: The dataset needs to be divided into two components, x and y. The columns in X will be 1 and 2, and the columns in Y will be the two columns. \n",
    "\n",
    "Step 3: The linear regression model must be fitted into two components. \n",
    "\n",
    "Step 4: The polynomial regression model needs to be fitted in two components: x and y. \n",
    "\n",
    "Step 5: With the help of a scatter plot, one will visualize linear regression results. \n",
    "\n",
    "Step 6: The polynomial regression will also be viewed in this step using a scatter plot. \n",
    "\n",
    "Step 7: New results will now be predicted using Linear and Polynomial regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aadf72dd-7456-45a0-8223-820132a36f0a",
   "metadata": {},
   "source": [
    "## Q8. What are the advantages and disadvantages of polynomial regression compared to linear\n",
    "# regression? In what situations would you prefer to use polynomial regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d3c47e-f746-41c0-be44-9a48a57e82dc",
   "metadata": {},
   "source": [
    "Advantage – Polynomial Regression\n",
    "The polynomial regression is flexible enough to get fitted in a vast range of curvatures. \n",
    "A broad range of functions can easily fit under it. \n",
    "\n",
    "The polynomial regression offers the best approximation of the relationship between the two dependent and independent variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea3dc74-9759-4834-b6ad-5762ef5054a6",
   "metadata": {},
   "source": [
    "Disadvantage – Polynomial Regression\n",
    "The presence of one or more outliers in the data can hurt the final results of the nonlinear analysis.\n",
    "\n",
    "The polynomial regression is very sensitive to the outliers. \n",
    "\n",
    "Very few model validation tools are available that help detect the outliers in nonlinear regression compared to the ones present for linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356be43b-02f7-46ef-8112-bae3f3f86318",
   "metadata": {},
   "source": [
    "## use of polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7f499f-56e7-4c0c-ad02-b1d6de75d185",
   "metadata": {},
   "source": [
    "Polynomial regression is only used when there is no linear correlation between the two variables. This is how it explains why it is more like the nonlinear functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a687c0-9af9-469d-a5d3-14b6893e6e05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
