{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "312e68f8-3325-42aa-a86f-39bee5298ae8",
   "metadata": {},
   "source": [
    "## Q1. What is boosting in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b515557-e032-49e0-aff1-c2821f0a07e1",
   "metadata": {},
   "source": [
    "Ans-Boosting is an ensemble modeling technique that attempts to build a strong classifier from the number of weak classifiers. It is done by building a model by using weak models in series. Firstly, a model is built from the training data. Then the second model is built which tries to correct the errors present in the first model. This procedure is continued and models are added until either the complete training data set is predicted correctly or the maximum number of models are added. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd47ad1d-c85c-4781-9b70-6dfcb1c9e98c",
   "metadata": {},
   "source": [
    "## Q2. What are the advantages and limitations of using boosting techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00cc6a3-fac2-48e5-ac13-4947d92b73d8",
   "metadata": {},
   "source": [
    "AnsAdvantages of Boosting \n",
    "\n",
    "Improved Accuracy – Boosting can improve the accuracy of the model by combining several weak models’ accuracies and averaging them for regression or voting over them for classification to increase the accuracy of the final model.\n",
    "\n",
    "Robustness to Overfitting – Boosting can reduce the risk of overfitting by reweighting the inputs that are classified wrongly. \n",
    "\n",
    "Better handling of imbalanced data – Boosting can handle the imbalance data by focusing more on the data points that are misclassified \n",
    "\n",
    "Better Interpretability – Boosting can increase the interpretability of the model by breaking the model decision process into multiple processes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6728bf-c807-40ec-98b2-97e00159904d",
   "metadata": {},
   "source": [
    "## Q3. Explain how boosting works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce520321-2f48-4e38-b917-4f00f3323234",
   "metadata": {},
   "source": [
    "Ans-boosting combines weak learner a.k.a. base learner to form a strong rule.\n",
    "to find weak rule, we apply base learning (ML) algorithms with a different distribution. Each time base learning algorithm is applied, it generates a new weak prediction rule. This is an iterative process. After many iterations, the boosting algorithm combines these weak rules into a single strong prediction rule.\n",
    "For choosing the right distribution, here are the following steps:\n",
    "\n",
    "Step 1:  The base learner takes all the distributions and assign equal weight or attention to each observation.\n",
    "\n",
    "Step 2: If there is any prediction error caused by first base learning algorithm, then we pay higher attention to observations having prediction error. Then, we apply the next base learning algorithm.\n",
    "\n",
    "Step 3: Iterate Step 2 till the limit of base learning algorithm is reached or higher accuracy is achieved.\n",
    "\n",
    "Finally, it combines the outputs from weak learner and creates  a strong learner which eventually improves the prediction power of the model. Boosting pays higher focus on examples which are mis-classiﬁed or have higher errors by preceding weak rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92f729da-295a-473d-85b3-31ed7234e994",
   "metadata": {},
   "source": [
    "## Q4. What are the different types of boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d9ad1-bb55-4de1-94d6-4458b539d68b",
   "metadata": {},
   "source": [
    "Ans-Types of Boosting Algorithms\n",
    "Underlying engine used for boosting algorithms can be anything.  It can be decision stamp, margin-maximizing classification algorithm etc. There are many boosting algorithms which use other types of engine such as:\n",
    "\n",
    "AdaBoost (Adaptive Boosting)\n",
    "\n",
    "Gradient Tree Boosting\n",
    "\n",
    "XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d23fd51-794b-4ec1-9c50-3ae65fc79f2d",
   "metadata": {},
   "source": [
    "## Q5. What are some common parameters in boosting algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55cbb22-92c2-4b38-953b-c77cb08dbdc9",
   "metadata": {},
   "source": [
    "Ans-learning_rate. This determines the impact of each tree on the final outcome (step 2.4). ...\n",
    "\n",
    "n_estimators. The number of sequential trees to be modeled (step 2) ...\n",
    "\n",
    "subsample. The fraction of observations to be selected for each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb5b8ce-bd10-444e-a962-7887370488d7",
   "metadata": {},
   "source": [
    "## Q6. How do boosting algorithms combine weak learners to create a strong learner?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88d5f73-c007-491e-9bd3-0a5ea9135fd1",
   "metadata": {},
   "source": [
    "Ans-Boosting is an ensemble method that integrates multiple models(called as weak learners) to produce a supermodel (Strong learner).\n",
    "\n",
    "Basically boosting is to train weak learners sequentially, each trying to correct its predecessor. For boosting, we need to specify a weak model (e.g. regression, shallow decision trees, etc.), and then we try to improve each weak learner to learn something from the data.\n",
    "\n",
    "AdaBoost is a boosting algorithm where a decision tree with a single split is used as a weak learner. Also, we have gradient boosting and XG boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee96cbc-61ec-4255-941d-715e76acaf44",
   "metadata": {},
   "source": [
    "## Q7. Explain the concept of AdaBoost algorithm and its working."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec7af01-723c-4960-b830-17a31fa21b28",
   "metadata": {},
   "source": [
    "Ans-There are many machine learning algorithms to choose from for your problem statements. One of these algorithms for predictive modeling is called AdaBoost.\n",
    "\n",
    "AdaBoost, also called Adaptive Boosting, is a technique in Machine Learning used as an Ensemble Method. The most common estimator used with AdaBoost is decision trees with one level which means Decision trees with only 1 split. These trees are also called Decision Stumps.\n",
    "\n",
    "What this algorithm does is that it builds a model and gives equal weights to all the data points. It then assigns higher weights to points that are wrongly classified. Now all the points with higher weights are given more importance in the next model. It will keep training models until and unless a lower error is received."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708d482a-1219-484c-a927-fa749b62bc0e",
   "metadata": {},
   "source": [
    "## Q8. What is the loss function used in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39c7cd-c59d-417b-94df-56f1364d2260",
   "metadata": {},
   "source": [
    "Ans-The error function that AdaBoost uses is an exponential loss function. First we find the products between the true values of training samples and the overall prediction for each sample. Then we take the sum of all the exponentials of these products in order to compute the error at iteration m"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e587a2ac-7d0e-450f-a32f-b8d70abddd80",
   "metadata": {},
   "source": [
    "## Q9. How does the AdaBoost algorithm update the weights of misclassified samples?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a16d3f-b5e8-40b7-94b0-4635fd0643b7",
   "metadata": {},
   "source": [
    "Ans-New Sample Weight = Sample Weight * e^(Performance) \n",
    "\n",
    "In our case Sample weight = 1/5 so, 1/5 * e^ (0.693) = 0.399\n",
    "\n",
    "For correctly classified records, we use the same formula with the performance value being negative. This leads the weight for correctly classified records to be reduced as compared to the incorrectly classified ones. The formula is:\n",
    "\n",
    "New Sample Weight = Sample Weight * e^- (Performance)\n",
    "\n",
    "Putting the values, 1/5 * e^-(0.693) = 0.100\n",
    "\n",
    "The updated weight for all the records can be seen in the figure. As is known, the total sum of all the weights should be 1. In this case, it is seen that the total updated weight of all the records is not 1, it’s 0.799. To bring the sum to 1, every updated weight must be divided by the total sum of updated weight. For example, if our updated weight is 0.399 and we divide this by 0.799, i.e. 0.399/0.799=0.50. \n",
    "\n",
    "0.50 can be known as the normalized weight. In the below figure, we can see all the normalized weight and their sum is approximately 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637f06c0-d5a8-4e34-b033-eb8989f9f62a",
   "metadata": {},
   "source": [
    "## Q10. What is the effect of increasing the number of estimators in AdaBoost algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b27d950-a4e1-4414-af9b-1e9cc2d2dea7",
   "metadata": {},
   "source": [
    "Ans-Adaboost Attributes.\n",
    "estimators: The list of classifiers provided to be fit into the model.\n",
    "\n",
    "classes: The class labels.\n",
    "\n",
    "estimator_weights_: The weight assigned to each base estimator.\n",
    "\n",
    "estimator_errors_: Classification error for each estimator in the boosted ensemble.\n",
    "\n",
    "5. feature_importance_: Shows us which column has more importance than the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03cf2dd8-1cb8-40cb-8532-9be47438eed0",
   "metadata": {},
   "source": [
    "Hyperparameter tuning with Adaboost\n",
    "Let us play with the various parameters provided to us by the AdaBoost class and observer the accuracy changes:\n",
    "\n",
    "Explore the number of trees\n",
    "An important hyperparameter for Adaboost is n_estimator. Often by changing the number of base models or weak learners we can adjust the accuracy of the model. The number of trees added to the model must be high for the model to work well, often hundreds, if not thousands. Afterall the more is the number of weak learners, the more the model will change from being high biased to low biased."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc97f7d-bc09-4122-a1cb-5bd4f8910153",
   "metadata": {},
   "source": [
    "# explore adaboost ensemble number of trees effect on performance\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# get the dataset\n",
    "def get_dataset():\n",
    "\tX, y = make_classification(n_samples=1000, n_features=20, n_informative=15, n_redundant=5, random_state=6)\n",
    "\treturn X, y\n",
    "\n",
    "# get a list of models to evaluate\n",
    "def get_models():\n",
    "\tmodels = dict()\n",
    "\t# define number of trees to consider\n",
    "\tn_trees = [10, 50, 100, 500, 1000, 5000]\n",
    "\tfor n in n_trees:\n",
    "\t\tmodels[str(n)] = AdaBoostClassifier(n_estimators=n)\n",
    "\treturn models\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "\t# define the evaluation procedure\n",
    "\tcv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\t# evaluate the model and collect the results\n",
    "\tscores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "\treturn scores\n",
    "\n",
    "# define dataset\n",
    "X, y = get_dataset()\n",
    "# get the models to evaluate\n",
    "models = get_models()\n",
    "# evaluate the models and store results\n",
    "results, names = list(), list()\n",
    "for name, model in models.items():\n",
    "\t# evaluate the model\n",
    "\tscores = evaluate_model(model, X, y)\n",
    "\t# store the results\n",
    "\tresults.append(scores)\n",
    "\tnames.append(name)\n",
    "\t# summarize the performance along the way\n",
    "\tprint('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "# plot model performance for comparison\n",
    "pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5b8005-c284-4601-bfdf-af344f80713d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f6a93c-0ded-4866-b404-faf4cf207bca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ed7337-0cdc-4b70-8d82-cb87926054e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbab8509-f5c2-46e1-a8e3-fd5f4f68515a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
