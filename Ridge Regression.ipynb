{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03fe0aab-67df-42db-a337-2a5746923a1b",
   "metadata": {},
   "source": [
    "## Q1. What is Ridge Regression, and how does it differ from ordinary least squares regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc0bf8-520f-494b-86e7-c4d802f5e9d0",
   "metadata": {},
   "source": [
    "Ans-Ridge regression is a term used to refer to a linear regression model whose coefficients are estimated not by ordinary least squares (OLS), but by an estimator, called ridge estimator, that, albeit biased, has lower variance than the OLS estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e6973-634f-4ad7-871d-de0ce7b47eb3",
   "metadata": {},
   "source": [
    "## Q2. What are the assumptions of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9c2a91-6cfb-4b6f-849b-687c6df560b9",
   "metadata": {},
   "source": [
    "Assumptions of Ridge Regressions\n",
    "The assumptions of ridge regression are the same as that of linear regression: linearity, constant variance, and independence. However, as ridge regression does not provide confidence limits, the distribution of errors to be normal need not be assumed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77be9f6d-8d8d-4f71-9591-e9b60426120b",
   "metadata": {},
   "source": [
    "The regression has five key assumptions:\n",
    "Linear relationship.\n",
    "\n",
    "Multivariate normality.\n",
    "\n",
    "No or little multicollinearity.\n",
    "\n",
    "No auto-correlation.\n",
    "\n",
    "Homoscedasticity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e96b42-2e93-4379-8623-873fc8d83d18",
   "metadata": {},
   "source": [
    "import numpy as np   \n",
    "import pandas as pd\n",
    "import os\n",
    " \n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt   \n",
    "import matplotlib.style\n",
    "plt.style.use('classic')\n",
    " \n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df = pd.read_excel(\"food.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e095fe7-2fa4-4809-9898-eebf39d80cad",
   "metadata": {},
   "source": [
    "After conducting all the EDA on the data, treatment of missing values, we shall now go ahead with creating dummy variables, as we cannot have categorical variables in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20612e71-94fe-41cb-8d96-1286f1da0657",
   "metadata": {},
   "source": [
    "Where columns=cat is all the categorical variables in the data set.\n",
    "\n",
    "After this, we need to standardize the data set for the Linear Regression method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8e7072-3808-4d51-b308-611e0db6c469",
   "metadata": {},
   "source": [
    "## Q3. How do you select the value of the tuning parameter (lambda) in Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50181fcb-85ae-4c89-8d2e-0396a255c0ac",
   "metadata": {},
   "source": [
    "Ans-Selecting a good value for λ is critical. When λ=0, the penalty term has no effect, and ridge regression will produce the classical least square coefficients. However, as λ increases to infinite, the impact of the shrinkage penalty grows, and the ridge regression coefficients will get close zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6fda2b-2c92-4b71-aefe-7e6659e128f4",
   "metadata": {},
   "source": [
    "Choose a regularization method. For example: ...\n",
    "Use a sequence of tuning parameters to create a series of different models.\n",
    "Study the different models and select one that best fits your needs.\n",
    "\n",
    "0 to infinity\n",
    "L2 Regularization or Ridge regression\n",
    "\n",
    "The value of lambda can vary from 0 to infinity. One can observe that when the value of lambda is zero, the penalty term no longer impacts the value of the cost function and thus the cost function is reduced back to the sum of squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c14bc9a-0b07-4fcb-a3a7-532c7f72998f",
   "metadata": {},
   "source": [
    "## Q4. Can Ridge Regression be used for feature selection? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad9b54a-da0a-434e-b04a-0c6cd36a7f99",
   "metadata": {},
   "source": [
    "Ans-We can use ridge regression for feature selection while fitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad5d865-f5da-4699-b143-562f31f5b20e",
   "metadata": {},
   "source": [
    "Linear regression is a good model for testing feature selection methods as it can perform better if irrelevant features are removed from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b67f3d5-dbf8-4ca0-8580-49d7bf47c675",
   "metadata": {},
   "source": [
    "Ridge regression and Lasso regression are two popular techniques that make use of regularization for predicting.\n",
    "\n",
    "Both the techniques work by penalizing the magnitude of coefficients of features along with minimizing the error between predictions and actual values or records.\n",
    "\n",
    "The key difference however, between Ridge and Lasso regression is that Lasso Regression has the ability to nullify the impact of an irrelevant feature in the data, meaning that it can reduce the coefficient of a feature to zero thus completely eliminating it and hence is better at reducing the variance when the data consists of many insignificant features. Ridge regression, however, can not reduce the coefficients to absolute zero.\n",
    "\n",
    "Ridge regression performs better when the data consists of features which are sure to be more relevant and useful.\n",
    "\n",
    "mathematically, Lasso is = Residual Sum of Squares + λ * (Sum of the absolute value of the magnitude of coefficients).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea8f026-a59d-4980-a90d-acab2607fffa",
   "metadata": {},
   "source": [
    "Where,\n",
    "\n",
    "λ denotes the amount of shrinkage\n",
    "λ = 0 implies all features are considered and it is equivalent to the linear regression where only the residual sum of squares are considered to build a predictive model\n",
    "λ = ∞ implies no feature is considered i.e, as λ closes to infinity it eliminates more and more features\n",
    "The bias increases with increase in λ\n",
    "variance increases with decrease in λ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e2344d-9088-41ff-86bb-ba8aa91ba5be",
   "metadata": {},
   "source": [
    "#import required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, LogisticRegression\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198444f6-67ea-4727-9c25-766386db13d3",
   "metadata": {},
   "source": [
    "# Define the headers since the data does not have any\n",
    "headers = [“over_draft”, “credit_usage”, “credit_history”, “purpose”,\n",
    "“current_balance”, “Average_Credit_Balance”, “employment”, “location”,\n",
    "“personal_status”, “other_parties”, “residence_since”, “property_magnitude”, “cc_age”, “other_payment_plans”, “housing”,\n",
    "“existing_credits”, “job”, “num_dependents”, “own_telephone”, “foreign_worker”, “target” ]\n",
    "#import dataset into the directory\n",
    "data = pd.read_csv(‘germandata.csv’, header=None, names=headers, na_values=”?” )\n",
    "\n",
    "numerics = [‘int16’,’int32',’int64',’float16',’float32',’float64']\n",
    "numerical_vars = list(data.select_dtypes(include=numerics).columns)\n",
    "data = data[numerical_vars]\n",
    "data.shape\n",
    "\n",
    "x = pd.DataFrame(data.drop(labels=[‘target’], axis=1))\n",
    "y= pd.DataFrame(data[‘target’])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e52cbb-aeab-40b5-8944-3be171d48fb6",
   "metadata": {},
   "source": [
    "To See Selected set of features\n",
    "\n",
    "selected_feat = X_train.columns[(sel_.get_support())]\n",
    "print(‘total features: {}’.format((X_train.shape[1])))\n",
    "print(‘selected features: {}’.format(len(selected_feat)))\n",
    "print(‘features with coefficients shrank to zero: {}’.format(\n",
    "np.sum(sel_.estimator_.coef_ == 0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e089ee-3279-4c04-890b-469f60deebcc",
   "metadata": {},
   "source": [
    "## Q5. How does the Ridge Regression model perform in the presence of multicollinearity?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4818c22-cae5-4fac-b769-751f947d9edf",
   "metadata": {},
   "source": [
    "Ans-When multicollinearity occurs, least squares estimates are unbiased, but their variances are large so they may be far from the true value. By adding a degree of bias to the regression estimates, ridge regression reduces the standard errors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d706d8-f81e-4cb5-b1bf-4e7b45222170",
   "metadata": {},
   "source": [
    "Ridge Regression is a technique for analyzing multiple regression data that suffer from multicollinearity, The particular kind used by ridge regression is known as L2 regularization . In ridge regression, the penalty is the sum of the squares of the coefficients. L2 Regularization aka Ridge Regularization — This add regularization terms in the model which are function of square of coefficients of parameters. Coefficient of parameters can approach to zero but never become zero and hence.\n",
    "\n",
    "Multicollinearity\n",
    "\n",
    "Multicollinearity, or collinearity, is the existence of near-linear relationships among the independent variables.\n",
    "\n",
    "Effects of Multicollinearity\n",
    "\n",
    "Multicollinearity can create inaccurate estimates of the regression coefficients, inflate the standard errors of the\n",
    "regression coefficients, deflate the partial t-tests for the regression coefficients, give false, nonsignificant, p-values, and degrade the predictability of the model (and that’s just for starters).Sources of Multicollinearity\n",
    "\n",
    "To deal with multicollinearity, you must be able to identify its source. The source of the multicollinearity impacts the analysis, the corrections, and the interpretation of the linear model. \n",
    "\n",
    "Data collection. In this case, the data have been collected from a narrow subspace of the independent\n",
    "variables. The multicollinearity has been created by the sampling methodology — it does not exist in the\n",
    "population. Obtaining more data on an expanded range would cure this multicollinearity problem. The\n",
    "extreme example of this is when you try to fit a line to a single point.\n",
    "Physical constraints of the linear model or population. This source of multicollinearity will exist no\n",
    "matter what sampling technique is used. Many manufacturing or service processes have constraints on\n",
    "independent variables (as to their range), either physically, politically, or legally, which will create\n",
    "multicollinearity.\n",
    "Over-defined model. Here, there are more variables than observations. This situation should be avoided."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cda1fae-84a0-4786-97cb-4dbb85d14a1d",
   "metadata": {},
   "source": [
    "Ridge Regression Models\n",
    "Following the usual notation, suppose our regression equation is written in matrix form as\n",
    "    \n",
    "\n",
    "where Y is the dependent variable, X represents the independent variables, B is the regression coefficients to be estimated, and e represents the errors are residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b70bf7e-bf81-4fee-a4f8-79f7e7746e9d",
   "metadata": {},
   "source": [
    "import mglearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "X, y = mglearn.datasets.load_extended_boston()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522cc55d-8cd6-4e32-ab1d-770e1a916867",
   "metadata": {},
   "source": [
    "Out:\n",
    "\n",
    "Training set score: 0.89\n",
    "Test set score: 0.75"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35378185-7b99-4ef0-bf1a-0de198d901d9",
   "metadata": {},
   "source": [
    "Ridge is a more restricted model, so this model overfit. A less complex model means worse performance on the training set, but if you over complex model, this is bad because can overfitt. How much importance the model places on simplicity versus training set performance can be specified by the user, using the alpha parameter. In the previous example, we used the default parameter alpha=1.0 . There is no reason why this will give us the best trade-off, though.\n",
    "The optimum setting of alpha depends on the particular dataset we are using.\n",
    "Increasing alpha forces coefficients to move more toward zero, which decreases training set performance but might help generalization. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9afb51ca-318c-4d9f-a0c9-a1ea9c466f7f",
   "metadata": {},
   "source": [
    "ridge10 = Ridge(alpha=10).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge10.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge10.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a376096-eeba-464c-b607-c2452ef18554",
   "metadata": {},
   "source": [
    "Out:\n",
    "\n",
    "Training set score: 0.79\n",
    "Test set score: 0.64\n",
    "\n",
    "Decreasing alpha allows the coefficients to be less restricted. For very small values of alpha , coefficients are barely restricted at all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea446c0c-6335-4670-b870-043e5756ba27",
   "metadata": {},
   "source": [
    "ridge01 = Ridge(alpha=0.1).fit(X_train, y_train)\n",
    "print(\"Training set score: {:.2f}\".format(ridge01.score(X_train, y_train)))\n",
    "print(\"Test set score: {:.2f}\".format(ridge01.score(X_test, y_test)))\n",
    "Out:\n",
    "\n",
    "Training set score: 0.93\n",
    "Test set score: 0.77\n",
    "\n",
    "We can get a more qualitative insight into how the alpha parameter changes the model by inspecting the coef_ attribute of models with different values of alpha . A higher alpha means a more restricted model, so we expect the entries of coef_ to have smaller magnitude for a high value of alpha than for a low value of alpha .for out comparision we use Linear Regresssion. This is confirmed in the plot in:\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression().fit(X_train, y_train)\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(ridge.coef_, 's', label=\"Ridge alpha=1\")\n",
    "plt.plot(ridge10.coef_, '^', label=\"Ridge alpha=10\")\n",
    "plt.plot(ridge01.coef_, 'v', label=\"Ridge alpha=0.1\")\n",
    "plt.plot(lr.coef_, 'o', label=\"LinearRegression\")\n",
    "plt.xlabel(\"Coefficient index\")\n",
    "plt.ylabel(\"Coefficient magnitude\")\n",
    "plt.hlines(0, 0, len(lr.coef_))\n",
    "plt.ylim(-25, 25)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c336117-c695-4235-9e41-50a7b5c06c14",
   "metadata": {},
   "source": [
    "## Q6. Can Ridge Regression handle both categorical and continuous independent variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311dd913-f147-45f2-a196-8947b877e2c4",
   "metadata": {},
   "source": [
    "Ans-Categorical variables require special attention in regression analysis because, unlike dichotomous or continuous variables, they cannot by entered into the regression equation just as they are. Instead, they need to be recoded into a series of variables which can then be entered into the regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553fe3be-96e3-4e2e-aad3-d5a3ca2a0808",
   "metadata": {},
   "source": [
    "## Q7. How do you interpret the coefficients of Ridge Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092c50d4-c484-404f-a005-a28f556025d0",
   "metadata": {},
   "source": [
    "A positive coefficient indicates that as the value of the independent variable increases, the mean of the dependent variable also tends to increase. A negative coefficient suggests that as the independent variable increases, the dependent variable tends to decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d45532-bd53-4214-8cb1-b04686dc6284",
   "metadata": {},
   "source": [
    "The slope is interpreted as the change of y for a one unit increase in x. This is the same idea for the interpretation of the slope of the regression line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398fc8e-4d1a-45a2-870c-35e9e4ac9aa6",
   "metadata": {},
   "source": [
    "Regression with One Predictor X\n",
    "\n",
    "If X sometimes equals 0, the intercept is simply the expected value of Y at that value. In other words, it's the mean of Y at one value of X. That's meaningful. If X never equals 0, then the intercept has no intrinsic meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0ce53f-c3ab-4737-b769-11a361173189",
   "metadata": {},
   "source": [
    "## Q8. Can Ridge Regression be used for time-series data analysis? If yes, how?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef78051-0a3c-4545-b9e5-4e4923004293",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "## training the model\n",
    "ridgeReg = Ridge(alpha=0.05, normalize=True)\n",
    "ridgeReg.fit(x_train,y_train)\n",
    "pred = ridgeReg.predict(x_cv)\n",
    "calculating mse\n",
    "mse = np.mean((pred_cv - y_cv)**2)\n",
    "mse 1348171.96 ## calculating score ridgeReg.score(x_cv,y_cv) 0.5691"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecb10ee-74b5-44ab-9ad7-7cdfc1108777",
   "metadata": {},
   "source": [
    "So, we can see that there is a slight improvement in our model because the value of the R-Square has been increased. Note that value of alpha, which is hyperparameter of Ridge, which means that they are not automatically learned by the model instead they have to be set manually.\n",
    "\n",
    "Here we have consider alpha = 0.05. But let us consider different values of alpha and plot the coefficients for each case."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8528e8-9c77-4ce7-9362-447176879ea1",
   "metadata": {},
   "source": [
    "You can see that, as we increase the value of alpha, the magnitude of the coefficients decreases, where the values reaches to zero but not absolute zero.\n",
    "But if you calculate R-square for each alpha, we will see that the value of R-square will be maximum at alpha=0.05. So we have to choose it wisely by iterating it through a range of values and using the one which gives us lowest error.\n",
    "\n",
    "So, now you have an idea how to implement it but let us take a look at the mathematics side also. Till now our idea was to basically minimize the cost function, such that values predicted are much closer to the desired result.\n",
    "\n",
    "Now take a look back again at the cost function for ridge regression.\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8690857c-deb2-4d73-8588-f137345e1408",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fe6968-36cf-4a4d-86b5-f826dcb4a46d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0002acb8-c97a-44d5-bc09-2c6a3db05be1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d644260-56bc-440d-8a92-ef8c33c32219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
