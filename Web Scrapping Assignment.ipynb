{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdb47ab3-69cc-48fe-8dd5-d6e639720516",
   "metadata": {},
   "source": [
    "## Q1. What is Web Scraping? Why is it Used? Give three areas where Web Scraping is used to get data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7954f271-8ef3-4030-9bfd-a40f17abe6ce",
   "metadata": {},
   "source": [
    "Ans-Web scraping is an automatic method to obtain large amounts of data from websites. Most of this data is unstructured data in an HTML format which is then converted into structured data in a spreadsheet or a database so that it can be used in various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b82b0bfb-ff72-4c80-83f7-c2df6612c6b2",
   "metadata": {},
   "source": [
    "Web scraping can help companies gather the correct contact information from their target marketâ€”including names, job titles, email addresses, and cellphone numbers. Then, they can reach out to these contacts and generate more leads and sales for their business."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd524bc-2635-450e-8605-d44730a65509",
   "metadata": {},
   "source": [
    "Web scraping is a technique for extracting data from websites using code or scripts. It can be a valuable skill for data collection, analysis, and automation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1d4738-cc5c-4d16-a5e9-09a86675b942",
   "metadata": {},
   "source": [
    "Price Monitoring. Web Scraping can be used by companies to scrap the product data for their products and competing products as well to see how it impacts their pricing strategies.\n",
    "\n",
    "Market Research.\n",
    "\n",
    "News Monitoring.\n",
    "\n",
    "Sentiment Analysis.\n",
    "\n",
    "Email Marketing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641efc11-63a0-4921-ac10-dcaa3e3cbcaf",
   "metadata": {},
   "source": [
    "## Q2. What are the different methods used for Web Scraping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaebddb-b137-4daa-89dc-bfb7fec58792",
   "metadata": {},
   "source": [
    "Ans-There are three main types of data scraping:\n",
    "\n",
    "Report mining: Programs pull data from websites into user-generated reports. \n",
    "\n",
    "Screen scraping: The tool pulls information on legacy machines into modern versions.\n",
    "\n",
    "Web scraping: Tools pull data from websites into reports users can customize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57fe0e2-5d72-4354-bf7f-bb1d289765c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8f8c521-672b-476f-a9af-d293edb1138c",
   "metadata": {},
   "source": [
    "## Q3. What is Beautiful Soup? Why is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ae73bf-85e3-400c-86f1-53a86f939e15",
   "metadata": {},
   "source": [
    "Ans-Beautiful Soup is a Python package for parsing HTML and XML documents (including having malformed markup, i.e. non-closed tags, so named after tag soup). It creates a parse tree for parsed pages that can be used to extract data from HTML, which is useful for web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd22f223-4766-4110-9d3e-61eac6496275",
   "metadata": {},
   "source": [
    "Step 1: Installation. Beautiful Soup can be installed using the pip command. ...\n",
    "\n",
    "Step 2: Inspect the Source. The next step is to inspect the website that you want to scrape. ...\n",
    "\n",
    "Step 3: Get the HTML Content. Next, get the HTML content from a web page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba2a5e-fbc7-48e1-9f6b-e1e4b444f3e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "99283561-7f0e-4968-83fe-6a13cdb3fcbf",
   "metadata": {},
   "source": [
    "## Q4. Why is flask used in this Web Scraping project?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847c2e73-8eff-4572-b9c5-ab23a7ff2c5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbee729b-3449-4d50-aeaf-0a659ee15605",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc31ee5-0620-469d-9463-b5a481f1dd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca9205ea-e377-4a3e-8394-ea254952f8b6",
   "metadata": {},
   "source": [
    "## Q5. Write the names of AWS services used in this project. Also, explain the use of each service."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fae4a67-6d39-4403-a4f5-dc30c0cb79c8",
   "metadata": {},
   "source": [
    "ANS-Data extraction services providers offer data extraction services and exports for businesses. Data extraction, sometimes also known as web scraping, is the scraping of data from a website or any other source like online forms, emails, and others using web scraping tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b81e2963-0527-426e-bded-7b20bc4b605a",
   "metadata": {},
   "source": [
    "Amazon EC2 (Elastic Compute Cloud):\n",
    "EC2 instances are virtual servers that can be used to run your web scraping scripts. You can choose the instance type based on your computing requirements and configure the environment as needed. EC2 instances provide the computing power required to execute the scraping scripts efficiently.\n",
    "\n",
    "Amazon S3 (Simple Storage Service):\n",
    "S3 can be used to store the scraped data. You can save the scraped data into S3 buckets, making it easy to manage and access the data. S3 also provides features like versioning and lifecycle policies, which can be useful for maintaining the data.\n",
    "\n",
    "Amazon RDS (Relational Database Service):\n",
    "If your project requires storing structured data, you could use RDS to set up a relational database. This can be particularly useful if you're scraping data that needs to be organized and queried efficiently.\n",
    "\n",
    "AWS Lambda:\n",
    "AWS Lambda can be used to run your scraping scripts in a serverless manner. Instead of managing servers, you can upload your code to Lambda and trigger it based on events or schedules. This can help in automating the scraping process and saving costs by executing code only when needed.\n",
    "\n",
    "Amazon CloudWatch:\n",
    "CloudWatch can be used for monitoring the health and performance of your scraping infrastructure. You can set up alarms, collect logs, and gain insights into the resource utilization of your EC2 instances and Lambda functions.\n",
    "\n",
    "Amazon DynamoDB:\n",
    "If your web scraping project deals with unstructured or semi-structured data, you might choose to use DynamoDB, a NoSQL database service. It's a good fit for storing JSON-like data and offers seamless scalability.\n",
    "\n",
    "AWS Step Functions:\n",
    "If your scraping process involves multiple steps or needs to be orchestrated, AWS Step Functions can help. It allows you to coordinate different services and steps in a visual workflow.\n",
    "\n",
    "Amazon API Gateway:\n",
    "If you need to expose your scraped data through APIs, API Gateway can be used to create and manage APIs that provide controlled access to your data.\n",
    "\n",
    "Amazon CloudFront:\n",
    "CloudFront can be used to distribute your scraped content globally with low-latency and high-speed delivery. This is particularly useful if you're serving the scraped content to users across different geographical regions.\n",
    "\n",
    "AWS Glue:\n",
    "AWS Glue is a managed ETL (Extract, Transform, Load) service that can help in preparing and transforming the scraped data before storing it in databases or data lakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1140c915-dbbb-4921-8a2f-b3bc5f632fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
