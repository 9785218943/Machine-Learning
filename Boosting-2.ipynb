{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d3b50b4-8556-4664-a48e-d2e91fbb1366",
   "metadata": {},
   "source": [
    "# Q1. What is Gradient Boosting Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f368a9-4e97-4749-854a-3c9951be5430",
   "metadata": {},
   "source": [
    "Ans=Gradient Boosting Regression is a machine learning technique used for both classification and regression tasks. It is an ensemble learning method that combines the predictions of several weak learners (typically decision trees) to create a strong predictive model. The term \"gradient boosting\" refers to the optimization technique used to minimize the loss function and improve the model's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdd5dd3-6138-4f5a-99f5-ba3f9ce1e044",
   "metadata": {},
   "source": [
    "# Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ef76d6-117b-4102-861f-e61f71f9ee89",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.models = []\n",
    "\n",
    "        # Initialize the model with the mean of the target variable\n",
    "        initial_prediction = np.mean(y)\n",
    "        self.models.append(initial_prediction)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute the residuals\n",
    "            residuals = y - self.predict(X)\n",
    "\n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=3)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the model with the new tree\n",
    "            self.models.append(tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using all the trees\n",
    "        predictions = np.sum(model.predict(X) for model in self.models)\n",
    "\n",
    "        # Multiply by the learning rate\n",
    "        return self.learning_rate * predictions\n",
    "\n",
    "# Generate a small dataset for demonstration\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 2 * X.squeeze() + np.random.normal(scale=2, size=100)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the gradient boosting model\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "print(f'R-squared: {r2}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17348d3-3031-414f-8848-9b6c647679f2",
   "metadata": {},
   "source": [
    "# Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f60c7012-b7ae-4d59-90d0-22d8ad51a73e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting Xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.1/297.1 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from Xgboost) (1.9.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from Xgboost) (1.23.5)\n",
      "Installing collected packages: Xgboost\n",
      "Successfully installed Xgboost-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9056da61-7ee1-4403-b9af-717655d9fce3",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot clone object '<__main__.GradientBoostingRegressor object at 0x7f12671d3ca0>' (type <class '__main__.GradientBoostingRegressor'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 17\u001b[0m\n\u001b[1;32m     14\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(gb_model, param_grid, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneg_mean_squared_error\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Fit the grid search to the data\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Print the best hyperparameters\u001b[39;00m\n\u001b[1;32m     20\u001b[0m best_params \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mbest_params_\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/model_selection/_search.py:789\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    786\u001b[0m cv_orig \u001b[38;5;241m=\u001b[39m check_cv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcv, y, classifier\u001b[38;5;241m=\u001b[39mis_classifier(estimator))\n\u001b[1;32m    787\u001b[0m n_splits \u001b[38;5;241m=\u001b[39m cv_orig\u001b[38;5;241m.\u001b[39mget_n_splits(X, y, groups)\n\u001b[0;32m--> 789\u001b[0m base_estimator \u001b[38;5;241m=\u001b[39m \u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    791\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs, pre_dispatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpre_dispatch)\n\u001b[1;32m    793\u001b[0m fit_and_score_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\n\u001b[1;32m    794\u001b[0m     scorer\u001b[38;5;241m=\u001b[39mscorers,\n\u001b[1;32m    795\u001b[0m     fit_params\u001b[38;5;241m=\u001b[39mfit_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose,\n\u001b[1;32m    802\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sklearn/base.py:79\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     74\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     75\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou should provide an instance of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     76\u001b[0m                 \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscikit-learn estimator instead of a class.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m             )\n\u001b[1;32m     78\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 79\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m     80\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot clone object \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m (type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m): \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     81\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mit does not seem to be a scikit-learn \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     82\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mestimator as it does not implement a \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     83\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mget_params\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m method.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mrepr\u001b[39m(estimator), \u001b[38;5;28mtype\u001b[39m(estimator))\n\u001b[1;32m     84\u001b[0m             )\n\u001b[1;32m     86\u001b[0m klass \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\n\u001b[1;32m     87\u001b[0m new_object_params \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot clone object '<__main__.GradientBoostingRegressor object at 0x7f12671d3ca0>' (type <class '__main__.GradientBoostingRegressor'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid to search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5]\n",
    "}\n",
    "\n",
    "# Create the gradient boosting regressor\n",
    "gb_model = GradientBoostingRegressor()\n",
    "\n",
    "# Instantiate the grid search with cross-validation\n",
    "grid_search = GridSearchCV(gb_model, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "best_params = grid_search.best_params_\n",
    "print(f'Best Hyperparameters: {best_params}')\n",
    "\n",
    "# Get the best model from the grid search\n",
    "best_gb_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_grid = best_gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "mse_grid = mean_squared_error(y_test, y_pred_grid)\n",
    "r2_grid = r2_score(y_test, y_pred_grid)\n",
    "\n",
    "print(f'Mean Squared Error (Grid Search): {mse_grid}')\n",
    "print(f'R-squared (Grid Search): {r2_grid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df3f4e52-ad2b-47e4-88a1-ac552f0661ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "class GradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        self.models = []\n",
    "\n",
    "        # Initialize the model with the mean of the target variable\n",
    "        initial_prediction = np.mean(y)\n",
    "        self.models.append(initial_prediction)\n",
    "\n",
    "        for i in range(self.n_estimators):\n",
    "            # Compute the residuals\n",
    "            residuals = y - self.predict(X)\n",
    "\n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Update the model with the new tree\n",
    "            self.models.append(tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self)\n",
    "        X = check_array(X)\n",
    "\n",
    "        # Make predictions using all the trees\n",
    "        predictions = np.sum(model.predict(X) for model in self.models)\n",
    "\n",
    "        # Multiply by the learning rate\n",
    "        return self.learning_rate * predictions\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'n_estimators': self.n_estimators, 'learning_rate': self.learning_rate, 'max_depth': self.max_depth}\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self, parameter, value)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f89c60-9b74-41f5-b08b-83fea0326b34",
   "metadata": {},
   "source": [
    "# Q4. What is a weak learner in Gradient Boosting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f23d37d-9752-4d17-a3b6-ff0b019560dd",
   "metadata": {},
   "source": [
    "Ans=\n",
    "In the context of Gradient Boosting, a weak learner refers to a base model that performs slightly better than random chance on a given learning task. Specifically, it is a model that is better than random guessing but doesn't need to be highly accurate. Weak learners are typically simple models, often with low complexity, and are trained sequentially.\n",
    "\n",
    "In the case of Gradient Boosting, decision trees are commonly used as weak learners. These decision trees are often shallow, meaning they have a limited depth or few nodes. Shallow trees are less likely to overfit the training data and are easier to interpret.\n",
    "\n",
    "The key idea behind using weak learners in Gradient Boosting is to iteratively improve the model's performance by focusing on the mistakes made by the previous models. In each iteration, a new weak learner is trained to predict the residuals (the differences between the actual and predicted values) of the combined model. The weak learner's predictions are then scaled by a small learning rate and added to the ensemble of models.\n",
    "\n",
    "By sequentially combining weak learners and giving more emphasis to the areas where the model performs poorly, Gradient Boosting effectively builds a strong predictive model. The iterative nature of the process allows the algorithm to fit complex patterns in the data and achieve high accuracy.\n",
    "\n",
    "The use of weak learners is a key concept in boosting algorithms, and it contrasts with bagging methods (like Random Forests), where each base model is often a strong learner trained independently. Boosting methods, by contrast, build a strong model by learning from the mistakes of the previous models.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47312f36-88f1-4af0-8a08-582024a6ca57",
   "metadata": {},
   "source": [
    "# Q5. What is the intuition behind the Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6db33df8-c27e-48af-afa5-21f75570af5e",
   "metadata": {},
   "source": [
    "Ans=The intuition behind the Gradient Boosting algorithm lies in building a strong predictive model by combining the predictions of multiple weak learners, with each subsequent learner focusing on the mistakes of the ensemble built so far. The key steps involved in the algorithm provide the intuition:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "The algorithm starts with an initial prediction, often the mean or median of the target variable for regression tasks.\n",
    "For classification tasks, it can be initialized with the log-odds (for binary classification) or class probabilities.\n",
    "Sequential Learning:\n",
    "\n",
    "Weak learners (usually shallow decision trees) are sequentially added to the model.\n",
    "Each new weak learner is trained to correct the errors made by the combined model of the existing weak learners.\n",
    "Residuals and Gradients:\n",
    "\n",
    "In each iteration, the algorithm computes the residuals, which are the differences between the actual values and the predictions of the current ensemble.\n",
    "The new weak learner is then trained to predict these residuals.\n",
    "The learning process involves minimizing a loss function that quantifies the difference between the actual values and the current predictions.\n",
    "Weighted Sum of Weak Learners:\n",
    "\n",
    "The predictions of the weak learners are combined through a weighted sum.\n",
    "The weights are determined by a small learning rate, which is a hyperparameter set by the user.\n",
    "The learning rate controls the contribution of each weak learner to the overall model.\n",
    "Iterative Improvement:\n",
    "\n",
    "The process is repeated for a predefined number of iterations or until a specified level of performance is reached.\n",
    "At each iteration, the algorithm emphasizes the areas where the current model performs poorly.\n",
    "The intuition is that by focusing on the errors of the current model, Gradient Boosting iteratively improves its predictions. This sequential learning process allows the algorithm to fit complex patterns in the data, making it particularly powerful for tasks such as regression and classification.\n",
    "\n",
    "Overall, Gradient Boosting is a flexible and effective ensemble learning method that can be applied to a wide range of machine learning problems. It has proven to be successful in various domains due to its ability to handle non-linear relationships and capture intricate patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb71c2c-f171-4238-a3b9-0b27e80af64b",
   "metadata": {},
   "source": [
    "# Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc28ce7-97cb-430f-a672-b7733fc04506",
   "metadata": {},
   "source": [
    "Ans=The Gradient Boosting algorithm builds an ensemble of weak learners sequentially. The process involves iteratively adding weak learners to correct the errors or residuals made by the existing ensemble. Here's a step-by-step explanation of how the ensemble is built:\n",
    "\n",
    "Initialization:\n",
    "\n",
    "The algorithm starts with an initial prediction, often the mean (for regression) or log-odds (for binary classification) of the target variable.\n",
    "This initial prediction serves as the starting point for the ensemble.\n",
    "Compute Residuals:\n",
    "\n",
    "In each iteration, the algorithm computes the residuals, which are the differences between the actual target values and the current predictions of the ensemble.\n",
    "The residuals represent the errors made by the current model.\n",
    "Train Weak Learner on Residuals:\n",
    "\n",
    "A new weak learner, typically a shallow decision tree, is trained on the residuals.\n",
    "The weak learner is fitted to the negative gradient of the loss function with respect to the current ensemble's predictions.\n",
    "Update Ensemble with Weak Learner:\n",
    "\n",
    "The predictions of the new weak learner are added to the current ensemble.\n",
    "The contribution of the new weak learner is controlled by a small learning rate, which scales the predictions before they are added to the ensemble.\n",
    "Iterative Process:\n",
    "\n",
    "Steps 2-4 are repeated for a predefined number of iterations or until a specified level of performance is achieved.\n",
    "In each iteration, the weak learner focuses on the mistakes made by the current ensemble and contributes a correction to improve overall predictions.\n",
    "Final Prediction:\n",
    "\n",
    "The final prediction is obtained by summing the predictions of all weak learners in the ensemble, each weighted by the learning rate.\n",
    "The ensemble has effectively learned to correct its errors and make more accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90140ebe-72a3-41c5-8d01-e7805fba3522",
   "metadata": {},
   "source": [
    "# Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c71668-60f1-4498-81a0-ad8df32d420b",
   "metadata": {},
   "source": [
    "Ans=\n",
    "Constructing the mathematical intuition behind the Gradient Boosting algorithm involves understanding the optimization process, particularly how the algorithm minimizes a loss function by sequentially adding weak learners. Let's break down the key steps involved:\n",
    "\n",
    "Loss Function:\n",
    "\n",
    "Start with a loss function that measures the difference between the actual target values and the current predictions of the ensemble.\n",
    "For regression problems, the common loss functions include Mean Squared Error (MSE), and for classification problems, it might be Cross-Entropy Loss.\n",
    "Initialize Ensemble:\n",
    "\n",
    "Initialize the ensemble with a simple model, often the mean or median of the target variable for regression tasks, or log-odds for binary classification.\n",
    "Compute Pseudo-Residuals:\n",
    "\n",
    "Calculate pseudo-residuals, which are the negative gradients of the loss function with respect to the current predictions of the ensemble.\n",
    "Pseudo-residuals represent the errors made by the current model and guide the training of the next weak learner.\n",
    "Train Weak Learner:\n",
    "\n",
    "Fit a weak learner (typically a decision tree) to the pseudo-residuals. The goal is to find a model that can correct the errors made by the current ensemble.\n",
    "Compute Learning Rate Weighted Prediction:\n",
    "\n",
    "Scale the predictions of the weak learner by a small learning rate.\n",
    "The learning rate controls the contribution of each weak learner to the overall model.\n",
    "Update Ensemble:\n",
    "\n",
    "Update the ensemble by adding the learning rate weighted predictions of the new weak learner.\n",
    "The ensemble now includes the contribution of the new weak learner.\n",
    "Repeat:\n",
    "\n",
    "Repeat steps 3-6 for a predefined number of iterations or until a specified level of performance is achieved.\n",
    "In each iteration, the weak learner focuses on the mistakes made by the current ensemble and contributes a correction.\n",
    "Final Prediction:\n",
    "\n",
    "The final prediction is obtained by summing up the predictions of all weak learners in the ensemble, each weighted by the learning rate.\n",
    "The ensemble has effectively learned to minimize the loss function and make accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b570c6-1c6f-4dad-be13-ce9496480a49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
