{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dbf306a5-0117-467f-9205-809ff890d93a",
   "metadata": {},
   "source": [
    "## Q1. How does bagging reduce overfitting in decision trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e15f70-d260-4ed6-abf4-282b6a3a4e5c",
   "metadata": {},
   "source": [
    "Ans-Bagging attempts to reduce the chance of overfitting complex models. It trains a large number of “strong” learners in parallel. A strong learner is a model that's relatively unconstrained. Bagging then combines all the strong learners together in order to “smooth out” their predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139a505f-59bb-4d34-8b0d-0d7f7a072681",
   "metadata": {},
   "source": [
    "## Q2. What are the advantages and disadvantages of using different types of base learners in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09141594-059f-47cc-b39f-13fc427d2cca",
   "metadata": {},
   "source": [
    "Ans-Bagging, also known as Bootstrap aggregating, is an ensemble learning technique that helps to improve the performance and accuracy of machine learning algorithms. It is used to deal with bias-variance trade-offs and reduces the variance of a prediction model. Bagging avoids overfitting of data and is used for both regression and classification models, specifically for decision tree algorithms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88c3c43c-0e9d-48ec-a67d-bc3ec21fb889",
   "metadata": {},
   "source": [
    "Steps to Perform Bagging\n",
    "\n",
    "Consider there are n observations and m features in the training set. You need to select a random sample from the training dataset without replacement\n",
    "\n",
    "A subset of m features is chosen randomly to create a model using sample observations\n",
    "\n",
    "The feature offering the best split out of the lot is used to split the nodes\n",
    "\n",
    "The tree is grown, so you have the best root nodes\n",
    "\n",
    "The above steps are repeated n times. It aggregates the output of individual decision trees to give the best prediction\n",
    "\n",
    "Advantages of Bagging in Machine Learning\n",
    "Bagging minimizes the overfitting of data\n",
    "\n",
    "It improves the model’s accuracy\n",
    "\n",
    "It deals with higher dimensional data efficiently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fcf8e03-7f27-42a7-b30a-dda8f459f1b6",
   "metadata": {},
   "source": [
    "Advantages and Disadvantages of Bagging\n",
    "Random forest is one of the most popular bagging algorithms. Bagging offers the advantage of allowing many weak learners to combine efforts to outdo a single strong learner. It also helps in the reduction of variance, hence eliminating the overfitting of models in the procedure.\n",
    "\n",
    "One disadvantage of bagging is that it introduces a loss of interpretability of a model. The resultant model can experience lots of bias when the proper procedure is ignored. Despite bagging being highly accurate, it can be computationally expensive, which may discourage its use in certain instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88624c98-3000-46f5-bd46-c4672e48b450",
   "metadata": {},
   "source": [
    "## Q3. How does the choice of base learner affect the bias-variance tradeoff in bagging?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbac2fab-e5c3-4fba-9cfc-957cf9df5e3a",
   "metadata": {},
   "source": [
    "Ans-While building the machine learning model, it is really important to take care of bias and variance in order to avoid overfitting and underfitting in the model. If the model is very simple with fewer parameters, it may have low variance and high bias. Whereas, if the model has a large number of parameters, it will have high variance and low bias. So, it is required to make a balance between bias and variance errors, and this balance between the bias error and variance error is known as the Bias-Variance trade-off.\n",
    "\n",
    "For an accurate prediction of the model, algorithms need a low variance and low bias. But this is not possible because bias and variance are related to each other:\n",
    "\n",
    "If we decrease the variance, it will increase the bias.\n",
    "If we decrease the bias, it will increase the variance.\n",
    "\n",
    "Bias-Variance trade-off is a central issue in supervised learning. Ideally, we need a model that accurately captures the regularities in training data and simultaneously generalizes well with the unseen dataset. Unfortunately, doing this is not possible simultaneously. Because a high variance algorithm may perform well with training data, but it may lead to overfitting to noisy data. Whereas, high bias algorithm generates a much simple model that may not even capture important regularities in the data. So, we need to find a sweet spot between bias and variance to make an optimal model.\n",
    "\n",
    "Hence, the Bias-Variance trade-off is about finding the sweet spot to make a balance between bias and variance errors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26196d63-044e-4ac9-9f2a-5e40a83229c8",
   "metadata": {},
   "source": [
    "## Q4. Can bagging be used for both classification and regression tasks? How does it differ in each case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9a626f-dcb0-4897-ab38-009139a0370f",
   "metadata": {},
   "source": [
    "Ans-Bagging is an ensemble method that can be used in regression and classification.\n",
    "It is also known as bootstrap aggregation, which forms the two classifications of bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0884f62b-0d41-406e-9704-cdd2b81cf6b8",
   "metadata": {},
   "source": [
    "## Q5. What is the role of ensemble size in bagging? How many models should be included in the ensemble?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b71539-05c8-487f-ab09-0560bf1856f5",
   "metadata": {},
   "source": [
    "Ans-Bagging is a powerful ensemble method which helps to reduce variance, and by extension, prevent overfitting. Ensemble methods improve model precision by using a group (or \"ensemble\") of models which, when combined, outperform individual models when used separately."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ad13da-4913-423a-800c-3520e964c15f",
   "metadata": {},
   "source": [
    "There are no restrictions/guidelines on the number of models. You can start even from 3 models. You can keep the number of models as a hyperparameter if the training cost is less."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1724c6b-da02-431c-aa7c-28828ff0ac82",
   "metadata": {},
   "source": [
    "## Q6. Can you provide an example of a real-world application of bagging in machine learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd22e372-e09e-4923-8a2e-d1ba57fda344",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv(\"house_prices.csv\")\n",
    "\n",
    "# separates independent (features) and dependent (prices) variables\n",
    "X = data.drop(\"price\", axis=1)\n",
    "y = data[\"price\"]\n",
    "\n",
    "# create the linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# fit the model to the data\n",
    "model.fit(X, y)\n",
    "\n",
    "# perform a prediction for a new set of features\n",
    "new_house= [[1500, 3, 2]]  # area, rooms, bathrooms\n",
    "price= model.predict(nova_casa)\n",
    "\n",
    "print(\"Expected price for the new house:\", price)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60ae9e7-74b1-449e-84a0-d8722b32c0eb",
   "metadata": {},
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n",
    "\n",
    "# create the logistic regression model\n",
    "logreg = LogisticRegression()\n",
    "\n",
    "# fit the model to the training data\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "# predict the target values for the test data\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "# print the accuracy score of the model\n",
    "print(\"Accuracy:\", logreg.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f83cb4d4-eaaf-4961-9f85-64b7a1289307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction for the new observation: [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# separate the features (independent variables) and target (dependent variable)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# create a Decision Tree classifier\n",
    "clf = DecisionTreeClassifier()\n",
    "\n",
    "# fit the model to the data\n",
    "clf.fit(X, y)\n",
    "\n",
    "# use the model to make predictions\n",
    "new_observation = [[5.2, 3.1, 4.2, 1.5]] # a new observation to predict\n",
    "prediction = clf.predict(new_observation)\n",
    "\n",
    "print(\"Prediction for the new observation:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9898e1-e65f-4e7a-816b-d9725fdc5d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class: [0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "# generate a random dataset\n",
    "X, y = make_classification(n_features=4, random_state=0)\n",
    "\n",
    "# create a random forest classifier with 100 estimators\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# fit the model to the data\n",
    "rf.fit(X, y)\n",
    "\n",
    "# predict the class of a new observation\n",
    "new_observation = [[-2, 2, -1, 1]]\n",
    "print(\"Predicted class:\", rf.predict(new_observation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fba5df52-c7cf-4e85-aef6-771ecee8591d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import svm\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "# Create an SVM classifier with a linear kernel\n",
    "clf = svm.SVC(kernel='linear')\n",
    "\n",
    "# Train the SVM classifier on the training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Print the accuracy of the classifier\n",
    "accuracy = clf.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdc2a5f6-5dba-4a9e-8df6-30dd51b42647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "import numpy as np\n",
    "\n",
    "# training data\n",
    "X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "Y = np.array([1, 1, 1, 2, 2, 2])\n",
    "\n",
    "# create Naive Bayes classifier and fit to the data\n",
    "clf = GaussianNB()\n",
    "clf.fit(X, Y)\n",
    "\n",
    "# make a prediction for a new data point\n",
    "new_point = [[0, 0]]\n",
    "prediction = clf.predict(new_point)\n",
    "\n",
    "print(\"Prediction:\", prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d45854d4-826c-4be0-ae04-81a686851957",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3)\n",
    "\n",
    "# create a kNN classifier with k=3\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "# fit the classifier to the training data\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# predict the classes of the testing set\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# print the accuracy of the classifier\n",
    "accuracy = knn.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d36b359b-5d6b-4999-9651-4ca6ed0bb632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# generate a random binary classification dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, random_state=42)\n",
    "\n",
    "# split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# create a gradient boosting classifier with default parameters\n",
    "clf = GradientBoostingClassifier()\n",
    "\n",
    "# train the model on the training data\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# calculate the accuracy of the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb288f4-ddf2-4ba9-a221-e462679887b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
