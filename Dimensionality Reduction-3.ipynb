{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76ee2979-68b1-4061-9d75-744ef84cc411",
   "metadata": {},
   "source": [
    "## Q1. What are Eigenvalues and Eigenvectors? How are they related to the Eigen-Decomposition approach? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098f08f2-f734-4955-9228-b5da17fb913c",
   "metadata": {},
   "source": [
    "Ans-Eigenvalues are the scalar values associated with the eigenvectors in linear transformation. The word ‘Eigen’ is of German Origin which means ‘characteristic’. Hence, these are the characteristic value that indicates the factor by which eigenvectors are stretched in their direction. It doesn’t involve the change in the direction of the vector except when the eigenvalue is negative. When the eigenvalue is negative the direction is just reversed. The equation for eigenvalue is given by\n",
    "\n",
    "Av = λv\n",
    "\n",
    "where A is the matrix\n",
    "\n",
    "v is associated eigenvector\n",
    "\n",
    "λ is scalar eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9967d3-669c-47e1-bebf-2782e8da9133",
   "metadata": {},
   "source": [
    "Eigenvectors for square matrices are defined as non-zero vector values which when multiplied by the square matrices give the scaler multiple of the vector, i.e. we define an eigenvector for matrix A to be “v” if it specifies the condition, Av = λv\n",
    "\n",
    "The scaler multiple λ in the above case is called the eigenvalue of the square matrix. We always have to find the eigenvalues of the square matrix first before finding the eigenvectors of the matrix.\n",
    "\n",
    "For any square matrix, A of order n × n the eigenvector is the column matrix of order n × 1. If we find the eigenvector of the matrix A by, Av = λv, “v” in this is called the right eigenvector of the matrix A and is always multiplied to the right-hand side as matrix multiplication is not commutative in nature. In general, when we find the eigenvector it is always the right eigenvector.\n",
    "\n",
    "We can also find the left eigenvector of the square matrix A by using the relation, vA = vλ\n",
    "\n",
    "Here, v is the left eigenvector and is always multiplied to the left-hand side. If matrix A is of order n × n then v is a column matrix of order 1 × n.\n",
    "\n",
    "Eigenvector Equation\n",
    "The Eigenvector equation is the equation that is used to find the eigenvector of any square matrix. The eigenvector equation is,\n",
    "Av = λv\n",
    "\n",
    "where,\n",
    "A is the given square matrix\n",
    "v is the eigenvector of matrix A\n",
    "λ is any scaler multiple"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6ecc9-a969-495d-a2bd-d12cff62fb3b",
   "metadata": {},
   "source": [
    "In linear algebra, eigendecomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05aa38d-ee67-4802-bbbf-ffcbf388015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)\n",
    "# calculate eigendecomposition\n",
    "values, vectors = eig(A)\n",
    "print(values)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3862817-4ce0-4a91-8a7d-e33420756079",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b20d1afd-d0a2-4821-848a-604fe2a88007",
   "metadata": {},
   "source": [
    "##  Q2. What is eigen decomposition and what is its significance in linear algebra?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623acea7-720b-46f4-8d02-c282fb075323",
   "metadata": {},
   "source": [
    "Ans-In linear algebra, eigendecomposition is the factorization of a matrix into a canonical form, whereby the matrix is represented in terms of its eigenvalues and eigenvectors. Only diagonalizable matrices can be factorized in this way."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f53a87-fe63-49a5-b43f-300eedacd0eb",
   "metadata": {},
   "source": [
    "Eigenvalues are associated with eigenvectors in Linear algebra. Both terms are used in the analysis of linear transformations. Eigenvalues are the special set of scalar values that is associated with the set of linear equations most probably in the matrix equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af01885-9aeb-499b-ac45-c795958c732e",
   "metadata": {},
   "source": [
    "## Q3. What are the conditions that must be satisfied for a square matrix to be diagonalizable using the\n",
    "# Eigen-Decomposition approach? Provide a brief proof to support your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8f8732-d96a-4aef-931f-15e1ca7be583",
   "metadata": {},
   "source": [
    "Ans- matrix is diagonalizable if and only if its nilpotent part is zero. Put in another way, a matrix is diagonalizable if each block in its Jordan form has no nilpotent part; i.e., each \"block\" is a one-by-one matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3e8c13-4bf8-425c-862e-97f7eac84d1d",
   "metadata": {},
   "source": [
    "Eigendecomposition is a technique used in Linear Algebra to break down a matrix into its constituent parts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de64dd40-cc00-4657-8839-f53aee87d72f",
   "metadata": {},
   "source": [
    "Let’s use a simple matrix to understand how Eigendecomposition is derived.\n",
    "\n",
    "Suppose we have a 2 x 2 square matrix A:\n",
    "\n",
    "Matrix\n",
    "\n",
    "Our first step is to determine its eigenvalues and eigenvectors. If you compute the eigenvalues, you should get:\n",
    "\n",
    "Eigenvalues\n",
    "\n",
    "The corresponding eigenvectors to these eigenvalues are:\n",
    "\n",
    "Eigenvectors\n",
    "\n",
    "When we normalize our eigenvectors, we should see the following output:\n",
    "\n",
    "Normalized-eigenvectors\n",
    "\n",
    "We can develop the following equations using these components because they explain eigenvalues and eigenvectors:\n",
    "\n",
    "Au1=λ1u1\n",
    "Au2=λ2u2\n",
    "If we compress these two systems into a matrix of eigenvectors and a matrix of eigenvalues, we get:\n",
    "\n",
    "Matrix-equation\n",
    "\n",
    "We can also use the following formula:\n",
    "\n",
    "Matrices\n",
    "\n",
    "When we use U\n",
    " and λ\n",
    " in equation (i)\n",
    ", we get the following equation:\n",
    "\n",
    "AU=UΛ\n",
    "To get our matrix A\n",
    " from this relationship, we take U\n",
    " inverse, i.e., U−1\n",
    ", on both sides of the matrix equation above. We end up with the equation below:\n",
    "\n",
    "A=UΛU−1\n",
    "This component is what we call Eigendecomposition. Matrix U\n",
    " contains eigenvectors and Λ\n",
    " has eigenvalues.\n",
    "\n",
    "Why is this matrix decomposition important?\n",
    "\n",
    "As stated, a matrix is a transformation that maps a vector from one point to another in the vector space.\n",
    "\n",
    "In machine learning algorithms, we often apply such transformations several times until the final output is obtained at each phase of the algorithm.\n",
    "\n",
    "However, the application also depends on the complexity of the problem. This means at the end, we will have taken our matrix raised to a power of a certain number.\n",
    "\n",
    "To understand this, suppose we have the following square matrix:\n",
    "\n",
    "AP\n",
    ", where p=16\n",
    "A simple approach to computing this matrix is to take the product of all possible pairs at each iteration.\n",
    "\n",
    "Let’s determine the number of iterations needed to compute this matrix until we get our final output as Ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ae3e4c5c-891f-48c7-a2b4-f6e38854dc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d41d8a6-c3cb-4ccb-911a-8a95d15599c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create our matrix\n",
    "M = np.array([[1,2,1], [0,1,0], [1,0,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fb39e42-9f02-4409-b5be-426a6001db6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 1]\n",
      " [0 1 0]\n",
      " [1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76b3b403-9d2e-4b1f-87cf-8127be512e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the eigenvalues and eigenvector of M\n",
    "Lambda, U =np.linalg.eig(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d04c481-22d4-4b01-8f30-2dd5ff3dfd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.07106781e-01 -7.07106781e-01  2.00093587e-17]\n",
      " [ 0.00000000e+00  0.00000000e+00  4.47213595e-01]\n",
      " [ 7.07106781e-01  7.07106781e-01 -8.94427191e-01]]\n"
     ]
    }
   ],
   "source": [
    "print(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "095560fe-161b-4f9d-bef4-ceb50329c927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(Lambda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da970700-0476-473b-a5f9-27bdad61a5db",
   "metadata": {},
   "source": [
    "We need Λ\n",
    " and U−1\n",
    " to calculate all components of our Eigendecomposition.\n",
    "\n",
    "We will first utilize the Numpy inv() function to determine U−1\n",
    " using the inverse of U\n",
    ", as demonstrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f89a895a-74af-4008-b3ca-6ed4a790b581",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.70710678,  1.41421356,  0.70710678],\n",
       "       [-0.70710678,  1.41421356,  0.70710678],\n",
       "       [ 0.        ,  2.23606798,  0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Gettng U inverse\n",
    "inv_U = np.linalg.inv(U)\n",
    "inv_U"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efed6649-5c67-4eeb-9c3e-04d03d041c5c",
   "metadata": {},
   "source": [
    "Let’s compute Λ\n",
    " on the diagonal matrix. We will use the diag() function with the Lambda vector to obtain this matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eb20e368-3380-470c-83db-f71cdb6bc36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 0., 0.],\n",
       "       [0., 0., 0.],\n",
       "       [0., 0., 1.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Λ = np.diag(Lambda)\n",
    "Λ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ce822-3dd3-4c5e-b6a1-87bd0c45c59a",
   "metadata": {},
   "source": [
    "Since we have the three components of our eigendecomposition, let’s reconstruct the original matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a65a1bf6-eeca-4918-b63c-accf02b82c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  2.,  1.],\n",
       "       [ 0.,  1.,  0.],\n",
       "       [ 1., -0.,  1.]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def round(values, decs=0):  # we don't want to include decimal points in our returned matrix\n",
    "  return np.round(values*10**decs)/(10**decs)\n",
    "\n",
    "vec = np.dot(U,np.dot(Λ, inv_U)) # taking the product of our three matrices\n",
    "round(vec, decs=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9944797-0a07-40e4-8a98-39745b1e576a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81670bd2-aec7-4ced-916e-a7face547f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d309ebe3-d661-4447-b70f-3f97a92fb610",
   "metadata": {},
   "source": [
    "## Q4. What is the significance of the spectral theorem in the context of the Eigen-Decomposition approach?\n",
    "# How is it related to the diagonalizability of a matrix? Explain with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b8c9eb-f7b7-48ad-9c8c-5b7fad42696e",
   "metadata": {},
   "source": [
    "Ans-Spectral Decomposition or time-frequency analysis (also time-frequency decomposition) is a method employed to aid in the interpretation of seismic data. Spectral decomposition can be performed on a multitude of attributes (frequency, dip, azimuth…), though the frequency is the most common.\n",
    "\n",
    "Is spectral decomposition same as eigen decomposition?\n",
    "Similarly, for the eigendecomposition (also known as eigenvalue decomposition, spectral decomposition, or diagonalization), I would say the following: An eigendecomposition describes the effect of a matrix A on a vector as a different 3-step process A=QΛQ−1: An invertible linear transformation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bced010-c6cb-4626-9427-220ae9881eef",
   "metadata": {},
   "source": [
    "The same methodology in math. An integer can be decomposed into prime factors, for example, 20 = 2 * 2 * 5, which means 20 is not divisible by 3, and any integer multiple of 20 will be divisible by 5.\n",
    "\n",
    "\n",
    "\n",
    "In mathematics, particularly linear algebra and functional analysis, a spectral theorem is a result about when a linear operator or matrix can be diagonalized (that is, represented as a diagonal matrix in some basis). This is extremely useful because computations involving a diagonalizable matrix can often be reduced to much simpler computations involving the corresponding diagonal matrix. The concept of diagonalization is relatively straightforward for operators on finite-dimensional vector spaces but requires some modification for operators on infinite-dimensional spaces. In general, the spectral theorem identifies a class of linear operators that can be modeled by multiplication operators, which are as simple as one can hope to find. In more abstract language, the spectral theorem is a statement about commutative C*-algebras. See also spectral theory for a historical perspective.\n",
    "\n",
    "Examples of operators to which the spectral theorem applies are self-adjoint operators or more generally normal operators on Hilbert spaces.\n",
    "\n",
    "The spectral theorem also provides a canonical decomposition, called the spectral decomposition, of the underlying vector space on which the operator acts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3384a5cc-0ef6-40ab-bd0d-934cedbc2d58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df8a76b-d3e7-47b5-a99e-e5e1125a633b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab9e3f64-f278-472e-a2cf-bd49c1460d22",
   "metadata": {},
   "source": [
    "## Q5. How do you find the eigenvalues of a matrix and what do they represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da8bebef-eeb9-431b-8d3e-26f3b788d802",
   "metadata": {},
   "source": [
    "Ans-Eigenvalues represent the scaling factor by which a vector is transformed when a linear transformation is applied, while eigenvectors represent the directions in which the transformation occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e95caf4-f589-4bf8-b21c-911db2df5892",
   "metadata": {},
   "source": [
    "Take the identity matrix I whose order is the same as A.\n",
    "\n",
    "Multiply every element of I by λ to get λI.\n",
    "\n",
    "Subtract λI from A to get A - λI.\n",
    "\n",
    "Find its determinant.\n",
    "\n",
    "Set the determinant to zero and solve for λ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131271d5-f59e-4e0d-becf-d40c52100f43",
   "metadata": {},
   "source": [
    "## Q6. What are eigenvectors and how are they related to eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3d262c-8c4e-4b5b-bb25-b8b39bd9011f",
   "metadata": {},
   "source": [
    "Ans-Eigenvalues represent the scaling factor by which a vector is transformed when a linear transformation is applied, while eigenvectors represent the directions in which the transformation occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6055263-461c-4e62-808e-213345a1dca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bebd886e-7c34-4dbb-8cf6-c4977587f156",
   "metadata": {},
   "source": [
    "## Q7. Can you explain the geometric interpretation of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b68013c-d538-4377-83d6-61baa7638691",
   "metadata": {},
   "source": [
    "Ans-Vectors can be represented geometrically by arrows (directed line segments). The arrowhead indicates the direction of the vector, and the length of the arrow describes the magnitude of the vector. →PQ,v,or→v. We often write v=→PQ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5131c0c8-4d70-489e-80ca-3930f045827a",
   "metadata": {},
   "source": [
    "An eigenvector, corresponding to a real nonzero eigenvalue for that matrix, points in a direction in which it is stretched by the transformation, and is neither rotated nor sheared. The eigenvalue is the factor by which an eigenvector is stretched. If the eigenvalue is negative, the direction is reversed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64707374-5a67-4f86-add4-1c25de01c776",
   "metadata": {},
   "source": [
    "## Q8. What are some real-world applications of eigen decomposition?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773fa023-dd87-4c28-bc26-04285d1a9894",
   "metadata": {},
   "source": [
    "Ans-Eigendecomposition provides us with a tool to decompose a matrix by discovering the eigenvalues and the eigenvectors. This operation can prove useful since it allows certain matrix operations to be easier to perform and it also tells us important facts about the matrix itself.\n",
    "\n",
    "Many applications of matrices in both engineering and science utilize eigenvalues and, sometimes, eigenvectors. Control theory, vibration analysis, electric circuits, advanced dynamics and quantum mechanics are just a few of the application areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3d776e-30f4-4f6d-bf8e-3df6ef3da549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ff95759-a6dd-452d-b6ce-cd1877e58a98",
   "metadata": {},
   "source": [
    "## Q9. Can a matrix have more than one set of eigenvectors and eigenvalues?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae597251-9f29-4726-a5a9-44ff70638066",
   "metadata": {},
   "source": [
    "Ans-Since a nonzero subspace is infinite, every eigenvalue has infinitely many eigenvectors. (For example, multiplying an eigenvector by a nonzero scalar gives another eigenvector.) On the other hand, there can be at most n linearly independent eigenvectors of an n × n matrix, since R n has dimension n ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454152d8-76bc-4532-bd14-34feb7ff56eb",
   "metadata": {},
   "source": [
    "The matrix A always has two eigenvalues, but sometimes they have algebraic multiplicity 2 or are complex numbers. If the matrix A has two complex eigenvalues, then it also has two linearly independent real eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2beb1bc-c4f0-4bc3-b413-ad4d431c2e33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5ebd52d1-44bf-40d3-ac50-8567c509bef4",
   "metadata": {},
   "source": [
    "## Q10. In what ways is the Eigen-Decomposition approach useful in data analysis and machine learning?\n",
    "# Discuss at least three specific applications or techniques that rely on Eigen-Decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ce448-e96f-4548-aace-42d9a8ff37b7",
   "metadata": {},
   "source": [
    "Ans-An eigendecomposition is calculated on a square matrix using an efficient iterative algorithm, of which we will not go into the details.\n",
    "\n",
    "Often an eigenvalue is found first, then an eigenvector is found to solve the equation as a set of coefficients.\n",
    "\n",
    "The eigendecomposition can be calculated in NumPy using the eig() function.\n",
    "\n",
    "The example below first defines a 3×3 square matrix. The eigendecomposition is calculated on the matrix returning the eigenvalues and eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "015e33c1-febb-47a2-8bd1-953784dfdce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]\n",
      " [7 8 9]]\n",
      "[ 1.61168440e+01 -1.11684397e+00 -1.30367773e-15]\n",
      "[[-0.23197069 -0.78583024  0.40824829]\n",
      " [-0.52532209 -0.08675134 -0.81649658]\n",
      " [-0.8186735   0.61232756  0.40824829]]\n"
     ]
    }
   ],
   "source": [
    "from numpy import array\n",
    "from numpy.linalg import eig\n",
    "# define matrix\n",
    "A = array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print(A)\n",
    "# calculate eigendecomposition\n",
    "values, vectors = eig(A)\n",
    "print(values)\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1477f8-eead-4512-9c2d-2b02b055afee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
